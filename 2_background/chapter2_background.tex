% -*- root: ../thesis.tex -*-
%!TEX root = ../thesis.tex
% ******************************* Thesis Chapter 2 ****************************


% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\graphicspath{{2/figures/}}
% ----------------------- contents from here ------------------------

A short introduction to the basic theory of \acl{GPs} and their scaling to larger is given in each paper.
However, in this chapter we go a bit more into detail on how inference and prediction can be computed.


\section{Probabilistic Bayesian Modeling}

\label{sec:prob_bayes}

The Bayes' theorem is one of the simplest theorem in probabilites and its demonstration holds in one line, yet its implications are very important.

Let's give the very general modeling setting.
Given a set of observed variables $\bx$, a set of latent (unobserved) variables $\btheta$ with a prior distribution $p(\btheta)$, and a likelihood function $p(\bx\mid\btheta)$, we can get the posterior distribution $p(\btheta|\bx)$:
\begin{align}
p(\btheta|\bx) = \frac{p(\bx|\btheta)p(\btheta)}{p(\bx)} = \frac{p(\bx|\btheta)p(\btheta)}{\int p(\bx|\btheta)p(\btheta)d\btheta}
\label{eq:bayes}
\end{align}

The interest of the posterior distribution is for making prediction on previously unseen data.
Let's take the simple example of logistic regression:
Given some input $\bx \in \mathbb{R}^D$ and binary label $y\in \{ 0, 1\}$ we model the generative model as:
\begin{align}
y \sim \Be\left(\sigma(\btheta^\top \bx)\right),
\end{align}
where $\btheta\in\mathbb{R}^D$ and $\sigma$ is the logistic function $\sigma(x) = \frac{1}{1+\exp(-x)}$.
For simplicity we use an isotropic Normal prior on $\btheta$ : $p(\btheta) = \No\left(\btheta|0, I_{D}\right)$ and use the following likelihood function: $p(y_i|\btheta,\bx_i) =\sigma\left(2(y_i - 1) \btheta^\top \bx_i\right)$.
Given the posterior $p(\btheta|\boldy, \bX)$ we can make predictions for new data using the following:
\begin{align}
p(y^*|\bX^*,\boldy,\bX) = \int p(y^*, \btheta|\bX^*\boldy, \bX)d\btheta = \int p(y^*|\btheta,\bX^*) p(\btheta|\boldy,\bX)d\btheta.
\end{align}
The last term of the equation involves the posterior distribution $p(\btheta|\by,\bX)$.
To solve this integral, we must either be able to know the posterior in closed form and solve the integral numerically, or be able to sample from it and compute this integral with Monte-Carlo integration.
Computing the posterior \eqref{eq:bayes} in closed-form involves computing the integral $\int p(\bx|\btheta)p(\btheta)d\btheta$, which is intractable for most non-trivial models.
In Section~\ref{sec:approx_inf}, we mention methods which help solving this issue by introducing approximations or ways to sample directly from the posterior.

\section{Gaussian Processes}

\ac{GP} are a class of stochastic processes used as non-parametric approximations to functions.
A \ac{GP} is a stochastic process $X_t$, where the joint distribution on any collection of variables $X_t$ follows a (multivariate) Gaussian distribution.
This Gaussian nature is what make them attractive since operations on Gaussian variables tend to be easier and many calculus have closed-form solutions.
The Gaussian distribution is to statistics what the harmonic oscillator is to physics.
Although, \ac{GPs} are defined to be a non-parametric model, one needs to define the covariance between each variable of the process.
One of the most popular interpretation of \ac{GP} is as a prior on functions in the \acf{RKHS}.
In practice the \ac{RKHS} is infinite-dimensional, to be able to perform any computation one needs to project it into a finite-dimensional space.
Considering a function $f$ we wish to approximate with a \ac{GP}, we need some data $\bX$ to evaluate $f$ on.
We then consider the finite-dimensional vector $\boldf$ where $f_i = f(X_i)$.


One resorts to kernel functions \com{need to cite this}.
The kernel matrix $K$ is defined by $K_{ij} = k(x_i, x_j)$.
$K$ is positive-definite, i.e. for $K\in \real^{D\times D}$, and $x\in \real^D$, $x^\top K x > 0$.

\subsection{Gaussian Process Regression}

We now have a prior on the realisation of the function $\boldf$ on some data $\bX$, $p(\boldf) = \No(\boldf|\bmu_0, \bK)$.
We can now add information about some noisy observations $\by$ we got for $\bX$:
\begin{align}
y_i = f(X_i) + \epsilon_i,
\end{align}
where $\epsilon_i \sim \No(0,\sigma^2)$.
This leads to the likelihood $p(y_i|f_i) = \No(y_i|f_i, \sigma^2)$.
Fortunately, multiplying Gaussian probability distributions together lead to another Gaussian distribution function.
The posterior for $\boldf$ is given by $p(\boldf|\by) = \No(\boldf|\by, \bK + \sigma^2 I)$.
The prediction of $f^*$ on a new point $\bx^*$ can be done by computing:
\begin{align}
p(f^*|\bx^*,\bX,\by) = \int p(f^*|\boldf,\bx^*)p(\boldf|\bX,\by)d\boldf.
\end{align}	
This integral is analytically solvable and results in $p(f^*|\bx^*,\bX,\by)=\No(f^*|m^*,s^*)$ where $m^* = K_{\bx^*,\bX}(K_{\bX,\bX} + \sigma^{2}I)^{-1}y$ and $s^*=K_{\bx^*,\bx^*} - K_{\bx^*,\bX}\left(K_{\bX,\bX}+\sigma^2I\right)^{-1}K_{\bX,\bx^*}$.

\subsection{Non-Conjugate Gaussian Processes}

A Gaussian prior is only conjugate\footnote{A prior is said conjugate to a given likelihood when the resulting posterior is of the same family of the prior.} to a Gaussian likelihood
Therefore \ac{GPs} only give a Gaussian posterior with a Gaussian likelihood, for all other cases we talk about \textit{Non-Conjugate Gaussian Processes}.

Since the posterior is not analytically tractable, one has to resort to some of the methods presented in Section~\ref{sec:approx_inf}.

\subsection{Sparse Gaussian Processes}
One of the largest issue of \ac{GPs}, regardless of if they are conjugate or not is the scalability with the number of observed samples.



\section{Approximate Bayesian Inference}
\label{sec:approx_inf}
The posterior distribution in Eq.\eqref{eq:bayes} cannot be computed in closed-form for non-trivial problems.
To still be able to make predictions and render the model useful one can resort to different approximations.
Out of a very large number of methods two of the most used are sampling and variational inference.

\subsection{Sampling}

When the posterior $p(\btheta|\bx)$ is not available in closed-form, it may be possible to draw samples from it.
The set of methods is far too large to be even mentioned in this thesis, I will restrict the scope to methods tailored or adapted to \ac{GPs}.
I will especially focus on \ac{MCMC} methods, where a chain of variable $\btheta^t$ is created with a Markovian assumption ($\btheta^t$ depends only of $\btheta^{t-1}$) and where the stationary distribution of $\btheta^t$ is the same as the target distribution (in our case the posterior $p(\btheta|\bx)$.



\subsection{Variational Inference}

\acf{VI}, sometimes called Variational Bayes, consists in approximating the posterior with another parametrized distribution.
Given a family of distributions $\mathcal{Q}$, parametrized by parameters $\bphi$ one aims to solve the following optimization problem:
\begin{align}
\bphi^* = \arg_{\bphi}\min \KL{q_{\bphi}(\btheta)}{p(\btheta|\bx)},
\label{eq:prob_VI}
\end{align}
where the $\mathrm{KL}$ (Kullback-Leibler) divergence is defined (for continuous distributions as:
\begin{align}
\KL{q(x)}{p(x)} = \int q(x) \log \frac{q(x)}{p(x)}dx
\end{align}

The objective of equation~\eqref{eq:prob_VI} is generally not tractable.
Since computing $p(\btheta|\bx)$ involves the typically intractable normalization constant $p(\bx)$, one resort to a surrogate function, the \ac{VFE} (or its negative counterpart the \ac{ELBO}):
\begin{align}
\KL{q_{\bphi}(\btheta)}{p(\btheta|\bx)} =& \int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\btheta|\bx)\right)d\btheta\\
=&\int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\btheta, \bx) - \log p(\bx)\right)d\btheta\\
=&\underbrace{- \log p(\bx)}_{\leq 0} + \int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\bx|\btheta)- \log p(\btheta) \right)d\btheta\\
\leq& -\expec{q_{\bphi}}{\log p(\bx|\theta)} + \KL{q_{\bphi}(\btheta)}{p(\btheta)} = \VFE(\bphi)
\end{align}


By minimizing the \ac{VFE}:$\VFE(\bphi)$ instead of the \ac{KL} divergence, we expect to find a solution close to the optimum of the problem stated in \eqref{eq:prob_VI}.
A standard way is to perform gradient descent on the variational parameters $\bphi$
\begin{align}
\bphi^{t+1} = \bphi^{t} - \epsilon \grad_{\bphi}\VFE(\bphi^{t}).
\end{align}

Computing the gradient $\grad_{\bphi}\VFE(\bphi)$ can be non-trivial but many methods were developed to tackle this problem.

\com{introduce different methods here}.
One of the most important addition to the \ac{VI} method is the \ac{MF} approximation.
\ac{MF} is the assumption that the variational distribution $q(\btheta)$ assumes every compoment of $\btheta$ to be independent from each other.
This way we can write
\begin{align}
q^{MF}_{\bphi}(\btheta) = \prod_{i=1}^D q_{\bphi_i}(\theta_i)
\end{align}
A more general method is also to consider blocks of variables instead.

Following the \ac{MF} approach, it is sometimes possible to find the optimal parameters $\bphi^*$ in closed-form.
By solving:
\begin{align}
\left.\grad_{\varphi_i}\VFE(\bphi)\right\vert_{\varphi_i=\varphi_i^*} = 0,
\end{align}
for each variable $\varphi_i$ we can find a local optima, which with additional assumptions, can prove to be the local minima we are looking for.
The advantage of this method is that one can also perform it independently for each latent variable $\theta_i$.
Concretely the updates are of the form:
\begin{align}
q_{\varphi_i}^*(\theta_i) \propto \exp\left(\expec{q_{\bphi}(\btheta_{/i})}{\log p\left(\theta_i|\btheta_{/i},\bx\right)}\right)
\end{align}
where $\btheta_{/i}$ represent the collection of variables $\btheta_{/i} = \{\theta_j | j \neq i\}$.
When working with distribution coming from exponential families, it is straightforward to get the optimal variational parameters $\varphi_i$.
By updating the parameters one after another we get a \ac{CAVI} scheme\footnote{The word ascent is used since the scheme was originally derived using the negative \ac{VFE} or \ac{ELBO}.}.
Effectively, one update each variational parameter $\varphi_i$ by its optimum given the rest of the variational parameters $\bphi_{/i}$ via closed-form functions:
\begin{align}
\varphi_i^{t+1} = f_i\left(\bphi_{1:(i-1)}^{t+1}, \bphi_{(i+1):D}^t\right).
\end{align}
The order of the updates do not matter as long as the variational parameters $\bphi$ are initialized in their domain.

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------