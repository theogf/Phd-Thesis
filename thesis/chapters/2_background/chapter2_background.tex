% -*- root: ../thesis.tex -*-
%!TEX root = ../thesis.tex
% ******************************* Thesis Chapter 2 ****************************


% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\graphicspath{{2/figures/}}
% ----------------------- contents from here ------------------------

A short introduction to the basic theory of \acl{GPs} as well as their extension to large datasets using inducing points \cite{Titsias2009} is given in each paper.
In this chapter, we go a bit more into detail on the basics of probabilistic Bayesian modeling, \acl{GPs}, \acl{VI} and sampling methods.


\section{Probabilistic Bayesian Modeling}

\label{sec:prob_bayes}

The Bayes' theorem is one of the simplest theorem in probabilities and its demonstration holds in one line, yet its implications are very important.

Let's give the very general modeling setting.
Given a set of observed variables $\bx$, a set of latent (unobserved) variables $\btheta$ with a prior distribution $p(\btheta)$, and a likelihood function $p(\bx\mid\btheta)$, we can get the posterior distribution $p(\btheta|\bx)$:
\begin{align}
p(\btheta|\bx) = \frac{p(\bx|\btheta)p(\btheta)}{p(\bx)} = \frac{p(\bx|\btheta)p(\btheta)}{\int p(\bx|\btheta)p(\btheta)d\btheta}
\label{eq:bayes}
\end{align}

The interest of the posterior distribution is for making prediction on previously unseen data.
Let's take the simple example of logistic regression:
Given some input $\bx \in \mathbb{R}^D$ and binary label $y\in \{ 0, 1\}$ we model the generative model as:
\begin{align}
y \sim \Be\left(\sigma(\btheta^\top \bx)\right),
\end{align}
where $\btheta\in\mathbb{R}^D$ and $\sigma$ is the logistic function $\sigma(x) = \frac{1}{1+\exp(-x)}$.
For simplicity, we use an isotropic Normal prior on $\btheta$ : $p(\btheta) = \No\left(\btheta|0, I_{D}\right)$ and use the following likelihood function: $p(y_i|\btheta,\bx_i) =\sigma\left(2(y_i - 1) \btheta^\top \bx_i\right)$.
Given the posterior $p(\btheta|\boldy, \bX)$ we can make predictions for new data using the following:
\begin{align}
p(y^*|\bX^*,\boldy,\bX) = \int p(y^*, \btheta|\bX^*\boldy, \bX)d\btheta = \int p(y^*|\btheta,\bX^*) p(\btheta|\boldy,\bX)d\btheta.
\end{align}
The last term of the equation involves the posterior distribution $p(\btheta|\by,\bX)$.
To solve this integral, we must either be able to know the posterior in closed form and solve the integral numerically, or be able to sample from it and compute this integral with Monte-Carlo integration.
Computing the posterior \eqref{eq:bayes} in closed-form involves computing the integral $\int p(\bx|\btheta)p(\btheta)d\btheta$, which is intractable for most non-trivial models.
In Section~\ref{sec:approx_inf}, we mention methods which help to solve this issue by introducing approximations or ways to sample directly from the posterior.

\section{Gaussian Processes}

\ac{GP} are a class of stochastic processes used as non-parametric approximations to functions.
A \ac{GP} is a stochastic process $X_t$, where the joint distribution on any collection of variables $X_t$ follows a (multivariate) Gaussian distribution.
This Gaussian nature is what make them attractive since operations on Gaussian variables tend to be easier and many calculi have closed-form solutions.
The Gaussian distribution is to statistics what the harmonic oscillator is to physics.
Although, \ac{GPs} are defined to be a non-parametric model, one needs to define the covariance between each variable of the process.
One of the most popular interpretation of \ac{GP} is as a prior on functions in the \acf{RKHS}.
In practice the \ac{RKHS} is infinite-dimensional, to be able to perform any computation one needs to project it into a finite-dimensional space.
Considering a function $f$ we wish to approximate with a \ac{GP}, we need some data $\bX$ to evaluate $f$ on.
We then consider the finite-dimensional vector $\boldf$ where $f_i = f(X_i)$.


One resorts to kernel functions \com{need to cite this}.
The kernel matrix $K$ is defined by $K_{ij} = k(x_i, x_j)$.
$K$ is positive-definite, i.e. for $K\in \real^{D\times D}$, and $x\in \real^D$, $x^\top K x > 0$.

\subsection{Gaussian Process Regression}

We now have a prior on the realization of the function $\boldf$ on some data $\bX$, $p(\boldf) = \No(\boldf|\bmu_0, \bK)$.
We can now add information about some noisy observations $\by$ we got for $\bX$:
\begin{align}
y_i = f(X_i) + \epsilon_i,
\end{align}
where $\epsilon_i \sim \No(0,\sigma^2)$.
This leads to the likelihood $p(y_i|f_i) = \No(y_i|f_i, \sigma^2)$.
Fortunately, multiplying Gaussian probability distributions together lead to another Gaussian distribution function.
The posterior for $\boldf$ is given by $p(\boldf|\by) = \No(\boldf|\by, \bK + \sigma^2 I)$.
The prediction of $f^*$ on a new point $\bx^*$ can be done by computing:
\begin{align}
p(f^*|\bx^*,\bX,\by) = \int p(f^*|\boldf,\bx^*)p(\boldf|\bX,\by)d\boldf.
\end{align}	
This integral is analytically solvable and results in $p(f^*|\bx^*,\bX,\by)=\No(f^*|m^*,s^*)$ where $m^* = K_{\bx^*,\bX}(K_{\bX,\bX} + \sigma^{2}I)^{-1}y$ and $s^*=K_{\bx^*,\bx^*} - K_{\bx^*,\bX}\left(K_{\bX,\bX}+\sigma^2I\right)^{-1}K_{\bX,\bx^*}$.

\subsection{Non-Conjugate Gaussian Processes}

A Gaussian prior is only conjugate\footnote{A prior is said conjugate to a given likelihood when the resulting posterior is of the same family of the prior.} to a Gaussian likelihood
Therefore \ac{GPs} only give a Gaussian posterior with a Gaussian likelihood, for all other cases we talk about \textit{Non-Conjugate Gaussian Processes}.

Since the posterior is not analytically tractable, one has to resort to some methods presented in Section~\ref{sec:approx_inf}.

\subsection{Sparse Gaussian Processes}
One of the largest issue of \ac{GPs}, regardless of if they are conjugate or not is the scalability with the number of observed samples.



\section{Approximate Bayesian Inference}
\label{sec:approx_inf}
The posterior distribution in Equation~\eqref{eq:bayes} cannot be computed in closed-form for non-trivial problems.
To make predictions, one can try nonetheless to approximate the posterior.
Many options exist, but this chapter will focus specifically on sampling and variational inference.

\subsection{Sampling}

Even if the posterior $p(\btheta|\bx)$ is not available in closed-form, it might be still possible to draw samples from it.
The whole collection of sampling techniques is far too large to be mentioned in this thesis.
Therefore, the scope will be restricted to methods tailored or adapted to \ac{GPs}.
We will especially focus on \ac{MCMC} methods, where a chain of variable $\btheta^t$ is created with the Markov assumption ($\btheta^t$ depends only on $\btheta^{t-1}$) and where the stationary distribution of $\btheta^t$ is the same as the target distribution (in our case the posterior $p(\btheta|\bx)$).

\subsubsection{Markov Chain Monte Carlo}

\subsubsection{Gibbs Sampling}

Gibbs sampling is a special case of \ac{MCMC} where each variable or block of variable is sampled one after another.
The proposal distribution is given by the full-conditional $p(\theta_i|\bx,\btheta_{/i})$.
The advantage of Gibbs sampling is that the Metropolis-Hastings step is guaranteed to accept the new samples since the acceptance rate is given by:
\begin{align*}
    A =& \frac{p(\theta^{t+1}_i,\btheta^t_{/i}|\bx)}{p(\theta^t_i,\btheta^t_{/i}|\bx)}\frac{p(\theta^t_i|\bx,\btheta^{t}_{/i})}{p(\theta^{t+1}_i|\bx,\btheta^t_{/i})}\\
    =& \frac{p(\theta^{t+1}|\bx, \btheta^{t}_{/i})}{p(\theta^t_i|\bx,\btheta^t_{/i})}\frac{p(\btheta^t_{/i}|\bx)}{p(\btheta^t_{/i}|\bx)}\frac{p(\theta^t_i|\bx,\btheta^{t}_{/i})}{p(\theta^{t+1}_i|\bx,\btheta^t_{/i})} = 1
\end{align*}

Gibbs sampling has several issues:
First, being able to sample and/or compute it is not necessarily given.
Second, it can suffer from heavy convergence issues if variables are heavily correlated, i.e. that the full-conditionals have tiny variances.
Some of these issues can be resolved by using additional techniques like "Blocked" Gibbs sampling where multiple variables are sampled at the same time or "Collapsed" Gibbs sampling where some conditional variables are marginalized out. 

\subsubsection{Hamilton/Hybrid Monte Carlo}

\acf{HMC} or Hybrid Monte Carlo \cite{betancourt2017conceptual} is a \ac{MCMC} gradient-based method.


\subsubsection{Elliptical Slice Sampling}


\subsection{Variational Inference}

\acf{VI}, sometimes called Variational Bayes, consists in approximating the posterior with another parametrized distribution.
Given a family of distributions $\mathcal{Q}$, parametrized by parameters $\bphi$ one aims to solve the following optimization problem:
\begin{align}
\bphi^* = \arg_{\bphi}\min \KL{q_{\bphi}(\btheta)}{p(\btheta|\bx)},
\label{eq:prob_VI}
\end{align}
where the $\mathrm{KL}$ \ac{KL} divergence is defined for continuous distributions as:
\begin{align}
\KL{q(x)}{p(x)} = \int q(x) \log \frac{q(x)}{p(x)}dx
\end{align}

The objective of equation~\eqref{eq:prob_VI} is generally not tractable.
Since computing $p(\btheta|\bx)$ involves the normalization constant $p(\bx)$, one resort to a surrogate function, the \ac{VFE} (or its negative counterpart the \ac{ELBO}):
\begin{align}
\KL{q_{\bphi}(\btheta)}{p(\btheta|\bx)} =& \int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\btheta|\bx)\right)d\btheta\\
=&\int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\btheta, \bx) - \log p(\bx)\right)d\btheta\\
=&\underbrace{- \log p(\bx)}_{\leq 0} + \int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\bx|\btheta)- \log p(\btheta) \right)d\btheta\\
\leq& -\expec{q_{\bphi}}{\log p(\bx|\theta)} + \KL{q_{\bphi}(\btheta)}{p(\btheta)} = \VFE(\bphi)
\end{align}


By minimizing the \ac{VFE}: $\VFE(\bphi)$ instead of the \ac{KL} divergence, we expect to find a solution close to the optimum of the problem stated in \eqref{eq:prob_VI}.
A standard way is to perform gradient descent on the variational parameters $\bphi$
\begin{align}
\bphi^{t+1} = \bphi^{t} - \epsilon \grad_{\bphi}\VFE(\bphi^{t}).
\end{align}

Computing the gradient $\grad_{\bphi}\VFE(\bphi)$ can be non-trivial, but many methods were developed to tackle this problem.

\subsubsection{Mean-Field Approximation}

One of the most important additions to the \ac{VI} method is the \ac{MF} approximation.
\ac{MF} is the assumption that the variational distribution $q(\btheta)$ assumes every component of $\btheta$ to be independent of each other.
This variational distribution can be specified as:
\begin{align}
q^{MF}_{\bphi}(\btheta) = \prod_{i=1}^D q_{\bphi_i}(\theta_i),
\end{align}
where $\bphi_i$ are the variational parameters for the variable $\theta_i$.

One can also build more general distributions by considering independence between blocks of variables instead.
We can split $\mathcal{I}=\{1,2,\ldots,D\}$, the set of indices of $\theta$, into $K$ independent subsets $\mathcal{I}_k \subseteq \mathcal{I}$ such that  $\mathcal{I} = \cup_{k=1}^K \mathcal{I}_{k}$ and $\mathcal{I}_i \cap \mathcal{I}_j=\emptyset,~\mathrm{iff}~i \neq j$.
The variational distribution based on this blocked mean-field approximation is then defined as
\begin{align}
    q^{BMF}_{\bphi}(\btheta) = \prod_{k=1}^K q_{\bphi_k}(\btheta_{\mathcal{I}_k}),
\end{align}
where $\bphi_k$ are the variational parameters for the set of variable $\theta_{\mathcal{I}_k}$.
\subsubsection{Coordinate Ascent VI}

Following the \ac{MF} approach, it is sometimes possible to find the optimal parameters $\bphi^*$ in closed-form.
By solving:
\begin{align}
\left.\grad_{\varphi_i}\VFE(\bphi)\right\vert_{\varphi_i=\varphi_i^*} = 0,
\end{align}
for each variable $\varphi_i$ we can find a local optima.
The advantage of this method is that one can also perform it independently for each latent variable $\theta_i$.
Concretely the updates are of the form:
\begin{align}
q_{\varphi_i}^*(\theta_i) \propto \exp\left(\expec{q_{\bphi}(\btheta_{/i})}{\log p\left(\theta_i|\btheta_{/i},\bx\right)}\right)
\end{align}
where $\btheta_{/i}$ represent the collection of variables $\btheta_{/i} = \{\theta_j | j \neq i\}$.
When working with distribution coming from exponential families, it is straightforward to get the optimal variational parameters $\varphi_i$.
By updating the parameters one after another we get a \ac{CAVI} scheme\footnote{The word ascent is used since the scheme was originally derived using the negative \ac{VFE} i.e. the \ac{ELBO}.}.
Effectively, one update each variational parameter $\varphi_i$ by its optimum given the rest of the variational parameters $\bphi_{/i}$ via closed-form functions:
\begin{align}
\varphi_i^{t+1} = f_i\left(\bphi_{1:(i-1)}^{t+1}, \bphi_{(i+1):D}^t\right).
\end{align}
The order of the updates do not matter as long as the variational parameters $\bphi$ are initialized in their domain.

The algorithm is presented completely on Algorithm~\ref{alg:CAVI}.

\begin{algorithm}
    \caption{\ac{CAVI} Updates}
    \label{alg:CAVI}
\end{algorithm}


\subsubsection{Natural Gradients}

One interesting aspect of \ac{CAVI}, is that it implicitly brings the concept of "Natural Gradients" \cite{amariNaturalGradientWorks1998}.
A natural gradient is a gradient with the inverse Fisher matrix applied as a preconditioner.
The Fisher information matrix is defined as 
\begin{align*}
    \mathcal{I}_\theta = \expec{p(\bx|\btheta)}{\left(\grad_{\btheta}\log p(\bx|\btheta)\right)\left(\grad_{\btheta} \log p(\bx|\btheta)\right)^\top} = -\expec{p(\bx|\btheta)}{\mathbf{H}(\log p(\bx|\btheta))},
\end{align*}
where $\mathbf{H}(f)$ is the Hessian matrix of the function $f$.
\com{Give more details about the meaning of the Fisher info}
The natural gradient is therefore given by :
\begin{align*}
    \widetilde{\grad}_{\bphi}\mathcal{F}(\bphi) = \mathcal{I}^{-1} \grad_{\bphi}\mathcal{F}(\bphi)
\end{align*}
This preconditioning of the gradients move the gradient descent in a different Riemannian metric.
The natural gradient is constructed such that the metric it gives maximizes the change of the \ac{KL} divergence between the given distribution and its target.
The reason why natural gradients are brought up here is that the updates of the \ac{CAVI} algorithm \ref{alg:CAVI} for exponential distributions, can be interpreted as natural gradient ascent updates with learning rate $1$.
\begin{align*}
    \phi^{t+1} = \phi^t + \mathcal{I}^{-1}\grad_{\phi}\mathcal{F}(\bphi^t) \equiv \phi^{t+1} = 
\end{align*}



% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------