% -*- root: ../thesis.tex -*-
%!TEX root = ../thesis.tex
% ******************************* Thesis Chapter 2 ****************************


% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\graphicspath{{2_background/figures/}}
% ----------------------- contents from here ------------------------

To fully comprehend the papers to be presented, we present a general overview of the needed concepts.
A short introduction to the basic theory of \aclp{GP} as well as their extension to large datasets using inducing points \cite{Titsias2009} is given in Chapters~\ref{ch:classification},~\ref{ch:multi-class} and \ref{ch:general}.
However, this chapter presents a more thorough and basic description.
Additionally, this chapter dives more into the basics of probabilistic Bayesian modeling, variational inference, and sampling methods.

\section{Probabilistic Bayesian Modeling}

\label{sec:prob_bayes}

Bayes' theorem is one of the simplest theorems in probability theory, and its proof fits in one line, yet its implications are immeasurably\footnote{Pun intended.} important.

Let us give a very general modeling setting that we will follow for the rest of this chapter.
Given a set of observed variables $\bX$, a set of latent (unobserved) variables $\btheta$ with a \textbf{prior} distribution $p(\btheta)$, and a \textbf{likelihood} function $p(\bX|\btheta)$, we obtain the \textbf{posterior} distribution $p(\btheta|\bX)$ via Bayes' theorem:
\begin{align}
p(\btheta|\bX) = \frac{p(\bX|\btheta)p(\btheta)}{p(\bX)} = \frac{p(\bX|\btheta)p(\btheta)}{\int p(\bX|\btheta)p(\btheta)d\btheta}.
\label{eq:bayes}
\end{align}

$p(\bX)$ represents the so-called evidence and can be used to compare different models (the dependency on the used model is implicit here).
The posterior allows us to obtain a distribution of the latent variables with its uncertainty given the prior $p(\btheta)$ and the data $\bX$.
The posterior is used for computing all kinds of expectations of the form $\expec{p(\btheta|\bX)}{f(\btheta)} = \int f(\btheta)p(\btheta|\bX)d\btheta$.
Expected values of interest can be statistics of the posterior like the mean ($\expec{p(\btheta|\bX)}{\btheta}$) or predictive distribution of new data points $p(\bx'|\bX) = \expec{p(\btheta|\bX)}{p(\bx'|\btheta)}$.

Let's take the simple example of linear logistic regression, a discriminative model.
Given an input $\bx \in \mathbb{R}^D$ and a binary label $y\in \{ 0, 1\}$, we model the process as:
\begin{align}
    y \sim \Be\left(\sigma(\btheta^\top \bx)\right),
\end{align}
where $\Be$ is the Bernoulli distribution, $\btheta\in\mathbb{R}^D$ is a vector of weights (our latent variable), and $\sigma : \mathbb{R} \to [0, 1]$ is the logistic function $\sigma(x) = \frac{1}{1 + \exp(-x)}$.
The likelihood function is given by: $p(y_i|\btheta,\bx_i) =\sigma\left(\btheta^\top \bx_i\right)^{y_i}\sigma\left(-\btheta^\top \bx_i\right)^{1-y_i}$.

Now let's suppose that we have $N$ pairs of input $\bx_i$ and label $y_i$, that we assume to be \ac{iid}, we get a training set $\bX = \{\bx_1,\ldots,\bx_N\}$, $\boldy = \{y_1,\ldots,y_N\}$.
With a prior distribution $p(\btheta)$ on $\btheta$, we build the posterior as $p(\btheta|\boldy,\bX) \propto p(\btheta)\prod_{i=1}^N p(y_i|\btheta, \bx_i)$.
We can then compute the predictive distribution for a new data input $\bx^*$:
\begin{align}
p(y^*|\bx^*,\boldy,\bX) = \int p(y^*, \btheta|\bx^*\boldy, \bX)d\btheta = \int p(y^*|\btheta,\bx^*) p(\btheta|\boldy,\bX)d\btheta.\label{eq:pred_dist}
\end{align}

Note that the last term of Equation~\eqref{eq:pred_dist} directly involves the posterior distribution $p(\btheta|\by,\bX)$.
To solve this integral, we must either know the posterior distribution and compute the integral numerically (or analytically) or sample from the posterior and estimate the integral using Monte Carlo integration.

\subsection{Posterior computations}
\label{sec:posterior}
Given a prior $p(\btheta)$ and a likelihood $p(\bx|\btheta)$, computing the posterior distribution function \eqref{eq:bayes} in closed-form requires the integral\footnote{Even if the integral is known, it might not be enough to compute some expectations or statistics.} $p(\bx)=\int p(\bx|\btheta)p(\btheta)d\btheta$.
For most non-trivial models, this integral is intractable, and approximations to the posterior are needed.
Such methods are introduced in Section~\ref{sec:approx_inf}.

However, in specific settings, computing the posterior in closed-form is possible.
When the prior is said to be \textbf{conjugate} to the likelihood, the posterior is of the same probability distribution family as the prior and is analytically tractable \cite{schlaifer1961applied}.
It is worth emphasizing this seemingly trivial case since it will be exploited in Section~\ref{sec:scale-mixtures}.
For a general example, we consider a likelihood part of the \textbf{exponential family}:
\begin{align}
    p(\bx|\btheta) = h(\bx)\exp(\boldeta(\btheta)^\top T(\bx) - A(\btheta)),
    \label{eq:explikelihood}
\end{align}
where $\btheta$ are the \textbf{distribution parameters}, $h(\bx)$ is the \textbf{base measure}, $\boldeta(\btheta)$ corresponds to the \textbf{natural parameters}, $T(\bx)$ are the \textbf{sufficient statistics} and $A(\btheta)$ is the \textbf{log-partition}.

Formally, a \textbf{conjugate prior} to the likelihood \eqref{eq:explikelihood} is defined as:
\begin{align}
    p(\btheta|\balpha) = h'(\btheta)\exp(\boldeta'(\balpha)^\top T'(\btheta) - A'(\balpha)),
    \label{eq:expprior}
\end{align}
where $T'(\btheta) = \left\{\boldeta(\btheta), A(\btheta)\right\}$ and where $\balpha$ is the prior distribution parameters.
Given a factorizable likelihood $p(\bX|\btheta) = \prod_{i=1}^N p(\bx_i|\btheta)$, the posterior will be proportional to
\begin{align}
    p(\btheta|\bX) \propto h'(\btheta)\exp\left(\left(\{\sum_{i=1}^N T(\bx_i), N\} + \boldeta'(\balpha)\right)^\top T'(\btheta)\right).\label{eq:posterior}
\end{align}
Note that the only dependence on $\bX$ is via the sufficient statistics $T(\bx)$.

Conjugate models are very practical as the posterior can be found in one step, but are very constraining in the choice of the prior.
They tend to be considered too simple for many applications.

If the prior is not conjugate of the likelihood, an alternative is to look for \textbf{conditional conjugacy}.
A parameter $\theta_i$ with a \textbf{conditionally conjugate prior} will have a full conditional distribution of the same family.
The full conditional distribution is defined as $p(\theta_i|
\bX, \btheta_{/i})$ where $\btheta_{/i} = \{\theta_1, \ldots, \theta_{i-1},\theta_{i+1},\ldots\theta_D\}$.
This notion of full conditional also extends to blocks of variables.
2In the same exponential family example in equation \eqref{eq:expprior}, we are interested in the conjugate prior to the distribution $p(\bX, \btheta_{/i} | \theta_i)$

\section{Gaussian Processes}
\label{sec:gps}

\acfp{GP} are a class of stochastic processes used as non-parametric probabilistic representations of functions.
A \ac{GP} is a stochastic process $\{f_t\}$, where the joint distribution on any finite collection of random variables $\{f_t\}$ follows a (multivariate) Gaussian distribution \cite{rasmussen2006gaussian}.
Since all the variables are Gaussian, we can perform all linear operations analytically, making them computationally attractive.
We can also compute marginals exactly, and a product of Gaussian distributions of the same variable is still proportional to another Gaussian.
% The Gaussian distribution is to statistics what the harmonic oscillator is to physics".

A \ac{GP} is uniquely specified by its \textbf{mean function} $\mu_0(\bx)$ and \textbf{kernel function} (also called covariance function) $k(\bx, \bx')$.
$\mu_0(\bx)$ can be any real-valued function while $k(\bx, \bx')$ needs to be a positive-definite function (also called Mercer kernels).
A symmetric function $k:\mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R} $ is positive-definite on $\mathcal{X}$ if $\boldsymbol{w}^\top K \boldsymbol{w}$ where $K_{ij} = K(\bx_i,\bx_j)$ for any $\boldsymbol{w}\in\mathbb{R}^N$ and any $\{\bx_i,\ldots,\bx_N\} \in \mathcal{X}$.

One of the interpretations of a \ac{GP}($\mu_0$, $k$) is as a prior on the function space.
% To perform concrete computations, we need to project it into a finite-dimensional space.
Given a random function $f$ we wish to represent with a \ac{GP}, we can project $f$ into a finite space by evaluating it on a set of data inputs $\bX = \{\bx_1, \ldots, \bx_N\}$ such that we obtain the finite-dimensional vector $\boldf$ where $f_i = f(\bx_i)$.
The prior on the projected \ac{GP} on $\bX$ is given by $\mathcal{N}\left(\bmu_0(\bX), K_{X}\right)$ where $\bmu_0(\bX) = \{\mu_0(\bx_i)\}_{i=1}^N$ and $K \in \mathbb{R}^{N\times N}$ is the kernel matrix, defined by $K_{ij} = k(\bx_i, \bx_j)$.

\subsection{Gaussian Process Regression}

Let us have a look at the most standard problem: Gaussian regression.
Given our prior $p(\boldf) = \No(\boldf|\bmu_0, \bK)$, we can add noisy observations $\by = \{y_i\}_{i=1}^N$ for each respective $\bx_i$ and model the process as:
\begin{align}
y_i = f(\bx_i) + \epsilon_i,
\end{align}
where $\epsilon_i \sim \No(0,\sigma^2)$.
This leads to the likelihood $p(y_i|f_i) = \No(y_i|f_i, \sigma^2)$.
Fortunately, adding a zero-mean Gaussian variable to another leads to another Gaussian distribution function with increased variance and the posterior for $\boldf$ is given by $p(\boldf|\by) = \No(\boldf|\by, \bK + \sigma^2 I)$.
The predictive distribution of $f_*=f(\bx_*)$ on a new input $\bx_*$ can be evaluated by computing:
\begin{align}
p(f_*|\bx_*,\bX,\by) = \int p(f_*|\boldf,\bx_*)p(\boldf|\bX,\by)d\boldf.
\end{align}	
This integral is analytically tractable and results in 
\begin{align}
    p(f_*|\bx_*,\bX,\by)=\No(f_*|m_*,s_*),
    \label{eq:pred_gp}
\end{align}
where $m_* = K_{\bx_*,\bX}(K_{\bX,\bX} + \sigma^{2}I)^{-1}\boldy$ and $s_*=K_{\bx_*,\bx_*} - K_{\bx_*,\bX}\left(K_{\bX,\bX}+\sigma^2I\right)^{-1}K_{\bX,\bx_*}$.
The predictive distribution for $f_*$ is Gaussian, with a known mean $m_*$ and a measure of uncertainty given by the variance $s_*$.
Note that $s_*$ depends directly on $K_{\bx_*,\bX}$: if $\bx_*$ is far from all points in $\bX$ (in the sense of the distance used in the kernel $k$), then $K_{\bx_*,\bX}$ will be very small and the variance $s_*$ maximized.
The predictive uncertainty will be high when new inputs $\bx_*$ are distant from the training data $\bX$.
A concrete example is shown on Figure~\ref{fig:gp_example}.

\begin{figure}
    \includegraphics[width=\textwidth]{./chapters/2_background/figures/GP_example.pdf}
    \caption{Illustration of the realization of a Gaussian Process. The black line is the true function $f$; the blue line is the mean of the prediction; the blue area represents the confidence interval of 2 standard deviations; the orange points represent observed data. Left: prediction on a grid given no observations. Right: prediction on a grid given a set of observations}
    \label{fig:gp_example}
\end{figure}

\subsection{Non-Conjugate Gaussian Processes}
\label{sec:nonconj_gps}
A Gaussian prior is only conjugate to the mean parameter of a Gaussian likelihood.
Therefore, the \ac{GP} posterior obtained in the previous section is only tractable for homoscedastic (the noise variance is independent of the input) Gaussian likelihoods, for all other cases we talk about \textbf{non-conjugate \acp{GP}}.
Examples of non-conjugate \ac{GP} problems are binary classification, regression using non-Gaussian noise such as Student-t or Laplace noise, or Poisson regression.
Other examples, such as multi-class classification or heteroscedastic regression, require multiple latent \acp{GP}.
Figure~\ref{fig:gp_class_example} shows an example of 1-dimensional binary classification with a \ac{GP} where the posterior was approximated using variational inference (see Section~\ref{sec:vi}).
Although the \ac{GP} does not recover exactly the true process, most of it is in the \ac{GP}'s 95\% confidence interval.

Posteriors of non-conjugate problems are not analytically tractable, and one needs to resort to the approximation methods presented in Section~\ref{sec:approx_inf}.
A strong focus of this thesis is to take these non-conjugate likelihoods and find a representation where inference is simplified and basic methods can be used.

\begin{figure}
    \includegraphics[width=\textwidth]{./chapters/2_background/figures/GP_classification_example.pdf}
    \caption{Illustration of a latent Gaussian process used for a binary classification problem.
    The Bernoulli likelihood is linked to the latent \ac{GP} via the logistic function.
    On the left is shown the optimal variational posterior $q(f)$ in blue, compared to the true generation of $f$ in green.
    Similar to Figure~\ref{fig:gp_example}, the blue band represents one standard deviation.
    On the right, we show the expected predictive probability for $y$ given the variational posterior $q(f)$ in blue.}
    \label{fig:gp_class_example}
\end{figure}

\subsection{Sparse Gaussian Processes}
\label{sec:sparsegps}
One of the largest drawbacks of \acp{GP}, regardless of the conjugacy of the likelihood, is the scalability with the number of observed samples.
When computing the predictive mean and covariance, the inverse matrix operation in Equation~\eqref{eq:pred_gp} has a computational complexity of $\mathcal{O}(N^3)$ where $N$ is the number of samples.
For one-dimensional inputs ($D=1$), solutions exist for specific kernels using state-space models representation \cite{pmlr-v9-turner10a,solinInfiniteHorizonGaussianProcesses2018}, leading to an $\mathcal{O}(N)$ complexity.
However, higher-dimensional problems require alternative solutions.
The first approach to reduce the complexity was to use a Nystr\"om approximation \cite{williams2002observations}.
\citet{csato2002sparse} proposed to create an approximation of the posterior using a subset of the points only in the context of online learning.
\citet{snelsonSparseGaussianProcesses2009} expanded this theory to the offline framework and \citet{csato2002gaussian} followed by \citet{Titsias2009} developed an alternative approximation based on KL divergence where the "inducing points" are not necessarily a subset of the training data and do not even have to belong to the same domain \cite{NIPS2009_5ea1649a, vdw2020framework}.
For a unified view on sparse \acp{GP}, see \citet{quinonero2005unifying} and \citet{bui2017unifying}.

The rest of this thesis is based on Titsias' approach \cite{Titsias2009}:
The sparse approximation is made by defining a set of inducing points location $Z=\{\boldz_i\}_{i=1}^M$ and the realization of a \ac{GP} $u$ on them: $\boldu$ where $u_i = u(\boldz_i)$.
We proceed to use variational inference (see Section~\ref{sec:vi}) and approximate the posterior $p(\boldu,\boldf|\boldy)$ by the variational distribution.
\begin{align}
    q(\boldu,\boldf) = q(\boldu)\prod_{i=1}^N p(f_i|\boldu).
    \label{eq:titsias_assumption}
\end{align}
The assumption used is that all components of the random vector $\boldf$ are independent of each other given the random vector $\boldu$.
It is a strong assumption, but the inference and prediction complexity reduces to $\mathcal{O}(NM^2 + M^3)$, where $N$ can be reduced to a smaller batch-size $B$ with stochastic inference approaches \cite{Hensman2013, Hensman2015}.
Given $q(\boldu) = \No(\bmu,\bSigma)$, the predictive distribution of $f_*=f(\bx_*)$ on a new input $\bx_*$ is given by
\begin{align*}
    p(f_*|\boldy,\bX) =& \int p(f_*|\boldu) p(\boldu|\boldy,\bX)d\boldu\\
    \approx& \int p(f_*|\boldu) q(\boldu)d\boldu\\
    =& p(f_*|m_*,s_*),
\end{align*}
where $m_* = K_{\bx_*,\bZ} K^{-1}_{\bZ,\bZ} \bmu$ and $s_* = K_{\bx_*,\bx_*} - K_{\bx_*,\bZ}K_{\bZ,\bZ}^{-1}(I - \Sigma) K^{-1}_{\bZ,\bZ}K_{\bZ,\bx_*}$.

\section{Approximate Bayesian Inference}
\label{sec:approx_inf}

The posterior distribution in Equation~\eqref{eq:bayes} cannot be computed in closed-form for non-trivial problems such as the ones presented in Section~\ref{sec:nonconj_gps} and \ref{sec:sparsegps}.
We can approximate the posterior to obtain a valuable estimator for predictions and expected values of interest.
\textbf{Approximate Bayesian Inference} is a research field of its own, and this chapter will focus specifically on sampling and \acl{VI}, the most popular approximate inference methods for \acp{GP}.

\subsection{Sampling}

Even if the posterior distribution $p(\btheta|\bX)$ is not available in closed-form, one can try to draw samples from it.
The advantage of sampling is its unbiasedness: one obtains exact expectations in the limit of infinitely many samples.
Sampling is an art of its own, and the number of methods is too large to mention them all in this thesis.
Therefore, the scope is restricted to methods popular with or tailored to \acp{GP}.
In particular, we restrict ourselves to \ac{MCMC} methods.

\paragraph{Markov Chain Monte Carlo and Metropolis-Hastings}\mbox{}\\
\acf{MCMC} methods generate a chain of variables $\btheta^t$ with the Markov assumption: $\btheta^t$ depends only on $\btheta^{t-1}$ and where the stationary distribution of $\btheta^t$ is the same as the target distribution $\pi(\btheta)$ (for our use case the posterior $p(\btheta|\bX)$).
\ac{MCMC} methods require a transition probability $t(\btheta^{t+1}|\btheta)$ which leaves the target stationary distribution invariant, i.e. $\pi(\btheta) = \int t(\btheta|\btheta')\pi(\btheta')d\btheta'$.
Other properties such as detailed balance and ergodicity need to be satisfied as well \cite{brooks2011handbook, o2004kendall}.

One of the most common algorithms to run a Markov Chain on a distribution $\pi(\btheta)$ is the \acf{MH} algorithm.
The \ac{MH} algorithm consists in having a proposal distribution $q(\btheta'|\btheta)$ suggesting a new sample.
Each proposed sample $\btheta'$ is randomly accepted or rejected with probability $p(\text{accept}) = \frac{\pi(\btheta')}{\pi(\btheta)}\frac{q(\btheta|\btheta')}{q(\btheta'|\btheta)} = A$.
The choice of the proposal distribution $q$ is the key to producing "good" chains with a high acceptance rate and a good exploration of $\btheta$'s parameter space.
Next are presented some categories of choice for the proposal $q$.


\paragraph{Gibbs Sampling}\mbox{}\\
Gibbs sampling is a particular \ac{MCMC} method where we sample each component of the random vector one after another.
The proposal distribution for each component is given by the full conditional  $p(\theta_i|\bx,\btheta_{/i})$, where $\btheta_{/i}=\{\theta_1, \ldots \theta_{i-1}, \theta_{i+1},\ldots \theta_{D}\}$.
The most prominent feature of Gibbs sampling is its acceptance probability, guaranteed to be 1:
\begin{align*}
    A =& \frac{p(\theta^{t+1}_i,\btheta^t_{/i}|\bx)}{p(\theta^t_i,\btheta^t_{/i}|\bx)}\frac{p(\theta^t_i|\bx,\btheta^{t}_{/i})}{p(\theta^{t+1}_i|\bx,\btheta^t_{/i})}\\
    =& \frac{p(\theta^{t+1}_i|\bx, \btheta^{t}_{/i})}{p(\theta^t_i|\bx,\btheta^t_{/i})}\frac{p(\btheta^t_{/i}|\bx)}{p(\btheta^t_{/i}|\bx)}\frac{p(\theta^t_i|\bx,\btheta^{t}_{/i})}{p(\theta^{t+1}_i|\bx,\btheta^t_{/i})} = 1.
\end{align*}
At every step, all proposed samples are therefore guaranteed to be accepted.
Also, since we are sampling from the full conditionals, regions of high density are reached pretty quickly.
Although it is not a guarantee, the sampler usually converges to the stationary distribution quickly.

We illustrate the path of the sampler on a two-dimensional bimodal example in Figure~\ref{fig:gibbs_samp}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{./chapters/2_background/figures/gibbs_sampling.pdf}
\caption{20 steps of the Gibbs sampler trajectory on the Rosenbrock distribution in 2 dimensions.}
\label{fig:gibbs_samp}
\end{figure}

The Gibbs sampling approach is a conundrum.
On the one hand, sampling each component using the full conditional is easy since it only involves drawing a scalar.
However, building a sampler for each full conditional at each step can be slow and costly.
The sampler can also get stuck or move very slowly if the components are highly correlated with another.
We can solve these drawbacks by using additional techniques like the \textbf{blocked Gibbs sampler} \cite{jensen1995blocking} where we sample groups of variables jointly, or \textbf{collapsed Gibbs sampling} \cite{liu1994collapsed} where we marginalize out some variables from the full conditional distributions.
But blocked or collapsed updates are much less available and require heavier sampling machinery.

The augmentations proposed in this thesis allow using both the blocked and collapsed version by deriving the full conditionals for each group of variables analytically.
Experiments show that the correlations are very low between each group of variables, and the sampler converges to the stationary distribution very fast.


\paragraph{Hamilton/Hybrid Monte Carlo}\mbox{}\\
\label{sec:hmc}
\acf{HMC} or Hybrid Monte Carlo \cite{duane1987hybrid, neal2011mcmc, betancourt2017conceptual} is a \ac{MCMC} method that uses Hamiltonian dynamics to make a new proposal.
We augment $\btheta^t$ with an extra momentum $\boldsymbol{p}^t$ sampled randomly for every proposal from $\No(0,M)$ where $M$ is the mass matrix.
Next we run the Hamiltonian dynamics based on the Hamiltonian $H(\btheta, \boldsymbol{p}) = -\log \pi(\btheta) + \frac{1}{2}\boldsymbol{p}^\top M \boldsymbol{p}$ over $L$ leapfrog steps with step size $\Delta t$.
The proposal at time $L\Delta t$ is accepted or rejected based on the acceptance rate:
\begin{align*}
    A = \min\left(1, \frac{\exp(-H(\btheta(L\Delta t), \boldsymbol{p}(L\Delta t)))}{\exp(-H(\btheta(0), \boldsymbol{p}(0)))}\right)
\end{align*}
Hamiltonian dynamics normally keep the Hamiltonian invariant.
However, symplectic (volume preserving) integrators, like the leapfrog method, only keep $H$ approximately invariant \cite{neal2011mcmc}.
The global error on $H$ grows as $\mathcal{O}(L(\Delta t)^2)$.
We get high acceptance rates while the dynamics lower the correlation between each sample by exploring the parameter space more freely than a basic random walk.
We can tune the \ac{HMC} algorithm parameters $M$, $L$, and $\Delta t$ by drawing a series of adaptive samples and by adjusting to the local geometry of the potential function $-\log\pi(\btheta)$.
Figure~\ref{fig:hmc} illustrates the sampling process with the Hamiltonian dynamics paths drawn with gray lines.

\ac{HMC} is very popular due to its plug-and-play characteristics but suffers from different issues.
It is gradient-based and can not sample discrete variables.
The integration of the Hamiltonian dynamics requires $2L$ gradients per proposal.
This computational cost can be prohibitively expensive for high-dimensional problems or $\pi(\btheta)$ distributions with costly computations.

\begin{figure}[H]
    \centering
\includegraphics[width=0.5\textwidth]{./chapters/2_background/figures/hmc_sampling.pdf}
\caption{Illustration of the HMC sampler (gray lines are the Hamiltonian dynamics)}
\label{fig:hmc}
\end{figure}


\paragraph{Other samplers}\mbox{}\\
There are other solid choices for sampling from \acp{GP}.
For example, elliptical slice sampling \citet{murray2010elliptical} is particularly well-fitted for Gaussian priors.
The \ac{NUTS} algorithm \cite{hoffman2014no} is an extension of \ac{HMC} where $L$ is chosen automatically.
We run the path integration with both $\boldsymbol{p}$ and $-\boldsymbol{p}$ until one of the particles goes backward or if one of the Hamiltonian estimates becomes too inaccurate\footnote{Technical details are skipped here.}.
The proposal is finally sampled randomly from both paths.
\ac{NUTS} is good at avoiding oscillatory dynamics and is particularly strong for quadratic problems, which appear regularly in \acp{GP} problems.
Finally, another orthogonal approach to sample from predictive distributions with a known \ac{GP} posterior is pathwise sampling \cite{wilson2021pathwise}.
By taking a mix of random Fourier features, specific to a particular class of kernels, the sampling complexity can be reduced from $\mathcal{O}(N^3)$ to $\mathcal{O}(T^3)$ where $N$ is the number of test inputs, and $T$ is the chosen number of basis.

\subsection{Variational Inference}
\label{sec:vi}
\acf{VI}, also called Variational Bayes, consists in approximating the posterior $p(\btheta|\bX)$ with another distribution $q(\btheta)$.
Given a family of distributions $\mathcal{Q}$, parametrized by the variational parameters $\bphi$, one aims to solve the following optimization problem:
\begin{align}
\bphi^* = \arg_{\bphi}\min \mathcal{D}\left(q_{\bphi}(\btheta), p(\btheta|\bx)\right),
\label{eq:prob_VI}
\end{align}
where $\mathcal{D}$ is a dissimilarity measure between two distributions and $q_{\bphi}$ is the distribution $q\in \mathcal{Q}$ parametrized by $\bphi$.
One of the most used dissimilarity measure is the backward \ac{KL} divergence, defined for continuous distributions as:
\begin{align}
\KL{q(x)}{p(x)} = \int q(x) \log \frac{q(x)}{p(x)}dx
\label{eq:kl}
\end{align}

The objective of Equation~\eqref{eq:prob_VI} or \eqref{eq:kl} is generally not directly tractable when the normalizer is not known.
Since $p(\btheta|\bx)$ involves the normalization constant $p(\bx)$, one resorts to a surrogate function, the \ac{VFE} (or its negative counterpart the \ac{ELBO}):
\begin{align}
\KL{q_{\bphi}(\btheta)}{p(\btheta|\bx)} =& \int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\btheta|\bx)\right)d\btheta\nonumber\\
=&\int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\btheta, \bx) - \log p(\bx)\right)d\btheta\nonumber\\
=&\underbrace{- \log p(\bx)}_{:=C} + \int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\bx|\btheta)- \log p(\btheta) \right)d\btheta\nonumber\\
=& C -\expec{q_{\bphi}}{\log p(\bx|\theta)} + \KL{q_{\bphi}(\btheta)}{p(\btheta)} = \VFE(\bphi) + C.\label{eq:vfe}
\end{align}


By minimizing $\VFE(\bphi)$ instead of the \ac{KL} divergence, we can expect to find a solution close to the optimum of the problem stated in Equation~\eqref{eq:prob_VI}.

A standard way to find the $\bphi^* = \arg_{\bphi}\min \VFE(\bphi)$ is to perform gradient descent on the variational parameters $\bphi$:
\begin{align}
\bphi^{t+1} = \bphi^{t} - \epsilon^t \grad_{\bphi}\VFE(\bphi^{t}),
\label{eq:vigraddescent}
\end{align}
where $\epsilon^t > 0$ is the learning rate.

Computing the gradient $\grad_{\bphi}\VFE(\bphi)$ can be non-trivial.
It involves derivatives over expectations, but "tricks" like reparametrization \cite{titsiasDoublyStochasticVariational} help to reduce the cost of these computations.

The choice of the family $\mathcal{Q}$ is a trade-off decision.
A richer, more complex family might be able to approximate the posterior better, but computing the \ac{KL} and optimizing the variational parameters will be increasingly difficult.
A standard example for continuous variables is the \ac{VGA}\footnote{The \ac{VGA} is explored in more details in Chapter~\ref{ch:gpf}.}, where the variational distribution $q_{\bphi}$ is a Gaussian, i.e. $\mathcal{Q} = \left\{q \sim \mathcal{N}(\boldm, S)\right\}$, and $\bphi = \{\boldm, S\}$.
Many expectations can be computed analytically under \ac{VGA}, in particular when the prior on $\btheta$ is Gaussian as well.
The Gaussian distribution is easily reparametrizable, and it is straightforward to sample from it.
Many operations will be of the cost $\mathcal{O}(D^3)$ where $D$ is the dimensionality of $\btheta$.
Restricting $\mathcal{Q}$ further by constraining $S$ can reduce this cost.
For example, setting $S$ to be diagonal will reduce the number of variational parameters and avoid inverse matrix operations.

\paragraph{Mean-Field Approximation}\mbox{}\\
We need assumptions on $\mathcal{Q}$ to reduce the computational cost of variational inference and scale with high-dimensional $\btheta$.
The \ac{MF} assumption imposes that every component of $\btheta$ is independent of each other.
A \ac{MF} variational family can be specified as:
\begin{align}
    \mathcal{Q}_{MF}= \left\{q = \prod_{i=1}^D q_{\bphi_i}(\theta_i)\right\},
\end{align}
where $\bphi_i$ are the variational parameters for the variable $\theta_i$.
Under the \ac{MF} approximation, the number of variational parameters grows linearly with the dimensionality of $\btheta$ instead of quadratically.
Additionally, integrals in Equation~\eqref{eq:vfe} can become one-dimensional or sometimes analytically tractable (the \ac{KL} for example), and therefore more easily solvable.
However, \ac{MF} can not capture potential posterior correlations between the components of $\btheta$.

An intermediate solution is to assume independence between blocks of variables instead, similarly to the blocked Gibbs sampler.
Given $\mathcal{I}=\{1,2,\ldots,D\}$, the set of indices of $\btheta$, we can build into $K$ independent subsets $\mathcal{I}_k \subseteq \mathcal{I}$ such that  $\mathcal{I} = \cup_{k=1}^K \mathcal{I}_{k}$ and $\mathcal{I}_i \cap \mathcal{I}_j=\emptyset,~\mathrm{iff}~i \neq j$.
The variational distribution based on this \ac{BMF} approximation is then defined as
\begin{align}
    q^{BMF}_{\bphi}(\btheta) = \prod_{k=1}^K q_{\bphi_k}(\btheta_{\mathcal{I}_k}),
\end{align}
where $\bphi_k$ are the variational parameters for the set of variable $\btheta_{\mathcal{I}_k}$.
The \ac{BMF} approximation can capture correlations inside blocks of variables but loses some of \ac{MF}'s computational attractiveness.

\paragraph{Coordinate Ascent VI}\mbox{}\\
\label{sec:cavi}
The \ac{CAVI}\footnote{The word ascent is used since the scheme was originally derived using the negative \ac{VFE}, i.e., the \ac{ELBO}.} approach is an alternative to the gradient descent approach of Equation~\eqref{eq:vigraddescent}.
Instead of moving all parameters at once in the gradient direction, we are interested in finding the optimal solution for each set of variational parameters $\bphi_i$ one after another by keeping the others fixed:
\begin{align}
    \bphi_i^* = \arg_{\bphi_i}\min \VFE(\bphi_i, \bphi_{/i}),
    \label{eq:optimalbphi}
\end{align}
where $\bphi_{/i} = \{\bphi_j | j \neq i\}$.
Using the \ac{BMF} approximation, we can update blocks of variational parameters at once.
The optimal $\bphi_i^*$ can be found by solving:
\begin{align}
\left.\grad_{\varphi_i}\VFE(\bphi)\right\vert_{\varphi_i=\varphi_i^*} = 0,
\end{align}
or performing a partial version of the gradient descent from Equation~\eqref{eq:vigraddescent}.
The solution to Equation~\eqref{eq:optimalbphi} is always given by
\begin{align}
q_{\bphi_i}^*(\btheta_i) \propto \exp\left(\expec{q_{\bphi}(\btheta_{/i})}{\log p\left(\btheta_i|\btheta_{/i},\bx\right)}\right)\label{eq:optdist_vi}
\end{align}
where $\btheta_{/i}$ represent the collection of variables $\btheta_{/i} = \{\theta_j | j \neq i\}$ \cite{murphyMachineLearningProbabilistic2012}.
Note that Equation~\eqref{eq:optdist_vi} is not always normalizable, but we are usually only interested in the different moments of $q_{\bphi_i}(\btheta_i)$.

Algorithm~\ref{alg:CAVI} summarizes the \ac{CAVI} algorithm.
The order of the updates does not matter as long as the variational parameters $\bphi$ are initialized in their respective domain.

\begin{algorithm}
    \caption{\ac{CAVI} algorithm}
    \label{alg:CAVI}
    \begin{algorithmic}
        \While {$|\VFE^{t+1} - \VFE^t| > \epsilon$}
            \For {$i \in \{1,\ldots,D\}$}
                \State $\bphi_i^{t+1} = \arg_{\bphi_i}\min \VFE(\bphi^{t+1}_{1:(i-1)},\bphi_i, \bphi^t_{(i+1):D})$,
            \EndFor
        \EndWhile
    \end{algorithmic}
\end{algorithm}

The \ac{CAVI} and Gibbs sampling algorithms are very similar in nature.
The observations on Gibbs sampling also apply: \ac{CAVI} updates on a distribution with \ac{MF} is easily computable but has slower convergence, while updates with the \ac{BMF} approach are more complex to derive, avoid some \ac{MF} pitfalls, and provide a richer distribution.

\paragraph{Natural Gradients}

One interesting aspect of \ac{CAVI}, is that it implicitly uses \textbf{natural gradients} \cite{amariNaturalGradientWorks1998}.
A natural gradient is a gradient preconditioned with the inverse Fisher information matrix defined as
\begin{align}
    \mathcal{I}_\theta = \expec{p(\bx|\btheta)}{\left(\grad_{\btheta}\log p(\bx|\btheta)\right)\left(\grad_{\btheta} \log p(\bx|\btheta)\right)^\top} = -\expec{p(\bx|\btheta)}{\mathbf{H}(\log p(\bx|\btheta))},
    \label{eq:fisherinfomatrix}
\end{align}
where $\mathbf{H}(f)$ is the Hessian matrix of the function $f$.
The Fisher information matrix is a Riemannian metric that gives the direction of the steepest descent with respect to the \ac{KL} divergence.
The natural gradient is given by :
\begin{align*}
    \widetilde{\grad}_{\bphi}\mathcal{F}(\bphi) = \mathcal{I}^{-1} \grad_{\bphi}\mathcal{F}(\bphi)
\end{align*}
The natural gradient works in a metric that maximizes the change of the infinitesimal \ac{KL} divergence between the given distribution and its target \cite{salimbeniNaturalGradientsPractice2018}.
The updates of the \ac{CAVI} algorithm \ref{alg:CAVI} for exponential distributions, can be interpreted as natural gradient ascent updates with learning rate $1$ \cite{wenzelEfficientGaussianProcess2018}.
\begin{align*}
    \bphi^{t+1} = \bphi^t + \mathcal{I}^{-1}_\theta\grad_{\bphi}\mathcal{F}(\bphi^t)
\end{align*}
When working with constrained parameters like the covariance matrix of the Gaussian variational distribution, a step with a high learning rate might overshoot out of the cone of positive-definite matrices.
\citet{salimbeniNaturalGradientsPractice2018} proposes a given schedule to compensate while \citet{linHandlingPositiveDefiniteConstraint2020} forces a trajectory on a geodesic.
Both approaches are computationally expensive, while we get this feature automatically.

\subsection{Scale mixtures and conditionally conjugate likelihoods}
\label{sec:scale-mixtures}
We base a large part of this work on mixtures and use scale mixtures in particular.
A scale mixture is a continuous mixture of a distribution with a varying scale parameter.
A textbook example is the Student-T distribution which is a Gaussian scale mixture with a Gamma prior on the variance:
\begin{align*}
    T_\nu(x) = \int_{0}^\infty \mathcal{N}\left(x|0,\omega\right)\mathrm{Ga}\left(\omega|\frac{\nu}{2}, \frac{\nu}{2}\right)d\omega,
\end{align*}
where $\mathrm{Ga}$ is a Gamma distribution.
Another example is the Laplace distribution which is also a Gaussian scale mixture:
\begin{align*}
    \mathrm{La}(x|\beta) = \int_0^{\infty} \mathcal{N}(x|0,\omega)\mathrm{Exp}\left(\omega|\frac{1}{2b^2}\right)d\omega,
\end{align*}
where $\mathrm{Exp}$ is the exponential distribution.

These representations appear when computing predictive distributions.
For example, when performing Gaussian linear regression with a fixed weight $\btheta$ and a Gamma prior on the likelihood variance $\sigma^2$, the resulting posterior predictive distribution will be a Student-T distribution.

This thesis shows that we can use this connection the other way around.
Certain likelihoods $p(\bx|\btheta)$ can be defined as scale mixtures $\int p(\bx|\btheta, \omega)p(\omega)d\omega$.
We can "unmarginalize" the likelihood by adding the scale variable $\omega$ to our model.
We  augment $p(\bx|\btheta)$ to $p(\bx,\omega|\btheta)$.
For example, we can augment a Student-T likelihood into a Gaussian likelihood with a Gamma prior on the variance.
The advantage of the augmented model is to produce conditionally conjugate likelihoods for all the model variables.

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------