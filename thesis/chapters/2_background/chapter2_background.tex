% -*- root: ../thesis.tex -*-
%!TEX root = ../thesis.tex
% ******************************* Thesis Chapter 2 ****************************


% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\graphicspath{{2/figures/}}
% ----------------------- contents from here ------------------------

A short introduction to the basic theory of \acl{GPs} as well as their extension to large datasets using inducing points \cite{Titsias2009} is given in each paper.
In this chapter, we go a bit more into detail on the basics of probabilistic Bayesian modeling, \acl{GPs}, \acl{VI} and sampling methods.


\section{Probabilistic Bayesian Modeling}

\label{sec:prob_bayes}

The Bayes' theorem is one of the simplest theorem in probabilities and its demonstration holds in one line, yet its implications are most important.

Let's give a very general modeling setting.
Given a set of observed variables $\bx$, usually the data, a set of latent (unobserved) variables $\btheta$ with a prior distribution $p(\btheta)$, and a likelihood function $p(\bx\mid\btheta)$, we obtain the posterior distribution $p(\btheta|\bx)$ via Bayes' theorem:
\begin{align}
p(\btheta|\bx) = \frac{p(\bx|\btheta)p(\btheta)}{p(\bx)} = \frac{p(\bx|\btheta)p(\btheta)}{\int p(\bx|\btheta)p(\btheta)d\btheta}
\label{eq:bayes}
\end{align}

The posterior allows us to obtain both an estimate of the latent variables of interest and the uncertainty of this estimate given the prior $p(\theta)$.
It also allows producing new data in a generative model or making predictions for new observations for a discriminative model.
Let's take the simple example of logistic regression, a discriminative model:
Given some input $\bx \in \mathbb{R}^D$ and binary label $y\in \{ 0, 1\}$ we model the generative model as:
\begin{align}
y \sim \Be\left(\sigma(\btheta^\top \bx)\right),
\end{align}
where $\Be$ is the Bernoulli distribution, $\btheta\in\mathbb{R}^D$ and $\sigma$ is the logistic function $\sigma(x) = \frac{1}{1+\exp(-x)}$, i.e. $\sigma : \mathbb{R} \mapsto [0, 1]$.
The likelihood function is then given by: $p(y_i|\btheta,\bx_i) =\sigma\left(\btheta^\top \bx_i\right)^{y_i}\sigma\left(-\btheta^\top \bx_i\right)^{1-y_i}$ for an output label $y_i$ and input features $\bx_i$.
Given $\bX = \{\bx_1,\ldots,\bx_N\}$, $\boldy = \{y_1,\ldots,y_N\}$, the training data and the corresponding posterior $p(\btheta|\boldy, \bX)$ we can make predictions for new input data $\bX^*$:
\begin{align}
p(y^*|\bX^*,\boldy,\bX) = \int p(y^*, \btheta|\bX^*\boldy, \bX)d\btheta = \int p(y^*|\btheta,\bX^*) p(\btheta|\boldy,\bX)d\btheta.
\end{align}
The last term of the equation involves the posterior distribution $p(\btheta|\by,\bX)$.
To solve this integral, we must either be able to know the posterior in closed form and solve the integral numerically (or analytically), or be able to sample from it and compute this integral using Monte-Carlo integration.
Computing the posterior \eqref{eq:bayes} in closed-form involves computing the integral $p(\bx)=\int p(\bx|\btheta)p(\btheta)d\btheta$, which is intractable for most non-trivial models.
In Section~\ref{sec:approx_inf}, we mention methods which help to solve this issue by introducing approximations or ways to sample directly from the posterior.

\section{Gaussian Processes}

\acf{GPs} are a class of stochastic processes used as non-parametric probabilistic representations to functions.
A \ac{GP} is a stochastic process $f_t$, where the joint distribution on any collection of variables $f_t$ follows a (multivariate) Gaussian distribution.
Their Gaussian nature makes them computationally very attractive.
Most operations on Gaussian variables can be done analytically, and they have other interesting properties (e.g. the marginal distribution of a component is also Gaussian). 
% The Gaussian distribution is to statistics what the harmonic oscillator is to physics".
\ac{GPs} are described to be a non-parametric, still one needs to define the covariance between each variable of the process.
\com{Missing transition}
One of the interpretation of a \ac{GP} is as a prior on functions in the \acf{RKHS}.
In practice the \ac{RKHS} is infinite-dimensional, and to be able to perform any computation one needs to project it into a finite-dimensional space.
Considering a function $f$ we wish to represent with a \ac{GP}, we can project it on data samples $\bX = \{\bx_i, \ldots, \bx_N\}$ and evaluate $f$ on them such that we obtain the finite-dimensional vector $\boldf$ where $f_i = f(X_i)$.
% \missingfigure{Put a prior of a GP}

One resorts to kernel functions \com{need to cite this}.
The kernel matrix $K$ is defined by $K_{ij} = k(x_i, x_j)$.
$K$ is positive-definite, i.e. for $K\in \real^{D\times D}$, and $x\in \real^D$, $x^\top K x > 0$.

\subsection{Gaussian Process Regression}

We now have a prior on the realization of the function $\boldf$ on the data $\bX = \{\bx_i\}_{i=1}^N$, $p(\boldf) = \No(\boldf|\bmu_0, \bK)$, where $\bmu_0 = \{\bmu_0(\bx_i)\}_{i=1}^N$ is the mean function.
We can add noisy observations $\by = \{y_i\}_{i=1}^N$ for each respective $\bX$:
\begin{align}
y_i = f(X_i) + \epsilon_i,
\end{align}
where $\epsilon_i \sim \No(0,\sigma^2)$.
This leads to the likelihood $p(y_i|f_i) = \No(y_i|f_i, \sigma^2)$.
Fortunately, multiplying Gaussian probability distributions together lead to another Gaussian distribution function.
The posterior for $\boldf$ is given by $p(\boldf|\by) = \No(\boldf|\by, \bK + \sigma^2 I)$.
The prediction of $f^*$ on a new point $\bx^*$ can be evaluated by computing:
\begin{align}
p(f^*|\bx^*,\bX,\by) = \int p(f^*|\boldf,\bx^*)p(\boldf|\bX,\by)d\boldf.
\end{align}	
This integral is analytically solvable and results in $p(f^*|\bx^*,\bX,\by)=\No(f^*|m^*,s^*)$ where $m^* = K_{\bx^*,\bX}(K_{\bX,\bX} + \sigma^{2}I)^{-1}\boldy$ and $s^*=K_{\bx^*,\bx^*} - K_{\bx^*,\bX}\left(K_{\bX,\bX}+\sigma^2I\right)^{-1}K_{\bX,\bx^*}$.

\subsection{Non-Conjugate Gaussian Processes}

A Gaussian prior is only conjugate\footnote{A prior is said conjugate to a given likelihood when the resulting posterior is of the same family of the prior.} to a Gaussian likelihood
Therefore \ac{GPs} only give a Gaussian posterior with a Gaussian likelihood, for all other cases we talk about \textit{Non-Conjugate Gaussian Processes}.

Since the posterior is not analytically tractable, one has to resort to some methods presented in Section~\ref{sec:approx_inf}.

\subsection{Sparse Gaussian Processes}
One of the largest issue of \ac{GPs}, regardless of their conjugacy, is the scalability with the number of observed samples.
When computing the covariance, the inverse operation has a computational complexity of $\mathcal{O}(N^3)$ where $N$ is the number of samples.
For one-dimensional inputs ($D=1$), solutions exist using state-space models representation, leading to an $\mathcal{O}(N)$ complexity but higher-dimensional problems require alternative solutions.
\citep{csato2002sparse} proposed first to create an approximation of the posterior using a subset of the points only in the context of online learning.
\citet{snelsonSparseGaussianProcesses2009} expanded this theory to the offline framework and \citet{Titsias2009} developed an alternative approximation based on KL divergence where the "inducing points" are not necessarily a subset of the training data and do not even have to belong to the same domain \com{cite related work}.

The rest of this thesis is based on Titsias approach, where the approximation is made by defining a set of inducing points location $Z=\{\boldz_i\}_{i=1}^M$ and the respective realization of $\boldu$ where $u_i = u(\boldz_i)$
\begin{align*}
    q(\boldu,\boldf) = q(\boldu)\prod_{i=1}^N p(f_i|\boldu)
\end{align*}


\section{Approximate Bayesian Inference}
\label{sec:approx_inf}
The posterior distribution in Equation~\eqref{eq:bayes} cannot be computed in closed-form for non-trivial problems.
To make predictions, one can try nonetheless to approximate the posterior.
Many options exist, but this chapter will focus specifically on sampling and variational inference.

\subsection{Sampling}

Even if the posterior $p(\btheta|\bx)$ is not available in closed-form, it might be still possible to draw samples from it.
The whole collection of sampling techniques is far too large to be mentioned in this thesis.
Therefore, the scope will be restricted to methods tailored or adapted to \ac{GPs}.
We will especially focus on \ac{MCMC} methods, where a chain of variable $\btheta^t$ is created with the Markov assumption ($\btheta^t$ depends only on $\btheta^{t-1}$) and where the stationary distribution of $\btheta^t$ is the same as the target distribution (in our case the posterior $p(\btheta|\bx)$).

\subsubsection{Markov Chain Monte Carlo}

\subsubsection{Gibbs Sampling}

Gibbs sampling is a special case of \ac{MCMC} where each variable or block of variable is sampled one after another.
The proposal distribution is given by the full-conditional $p(\theta_i|\bx,\btheta_{/i})$.
The advantage of Gibbs sampling is that the Metropolis-Hastings step is guaranteed to accept the new samples since the acceptance rate is given by:
\begin{align*}
    A =& \frac{p(\theta^{t+1}_i,\btheta^t_{/i}|\bx)}{p(\theta^t_i,\btheta^t_{/i}|\bx)}\frac{p(\theta^t_i|\bx,\btheta^{t}_{/i})}{p(\theta^{t+1}_i|\bx,\btheta^t_{/i})}\\
    =& \frac{p(\theta^{t+1}|\bx, \btheta^{t}_{/i})}{p(\theta^t_i|\bx,\btheta^t_{/i})}\frac{p(\btheta^t_{/i}|\bx)}{p(\btheta^t_{/i}|\bx)}\frac{p(\theta^t_i|\bx,\btheta^{t}_{/i})}{p(\theta^{t+1}_i|\bx,\btheta^t_{/i})} = 1
\end{align*}

Gibbs sampling has several issues:
First, being able to sample and/or compute it is not necessarily given.
Second, it can suffer from heavy convergence issues if variables are heavily correlated, i.e. that the full-conditionals have tiny variances.
Some of these issues can be resolved by using additional techniques like "Blocked" Gibbs sampling where multiple variables are sampled at the same time or "Collapsed" Gibbs sampling where some conditional variables are marginalized out. 

\subsubsection{Hamilton/Hybrid Monte Carlo}

\acf{HMC} or Hybrid Monte Carlo \cite{betancourt2017conceptual} is a \ac{MCMC} gradient-based method.


\subsubsection{Elliptical Slice Sampling}

% TODO Finish this part


\subsection{Variational Inference}

\acf{VI}, sometimes called Variational Bayes, consists in approximating the posterior with another parametrized distribution.
Given a family of distributions $\mathcal{Q}$, parametrized by parameters $\bphi$ one aims to solve the following optimization problem:
\begin{align}
\bphi^* = \arg_{\bphi}\min \KL{q_{\bphi}(\btheta)}{p(\btheta|\bx)},
\label{eq:prob_VI}
\end{align}
where the $\mathrm{KL}$ \ac{KL} divergence is defined for continuous distributions as:
\begin{align}
\KL{q(x)}{p(x)} = \int q(x) \log \frac{q(x)}{p(x)}dx
\end{align}

The objective of equation~\eqref{eq:prob_VI} is generally not tractable.
Since computing $p(\btheta|\bx)$ involves the normalization constant $p(\bx)$, one resort to a surrogate function, the \ac{VFE} (or its negative counterpart the \ac{ELBO}):
\begin{align}
\KL{q_{\bphi}(\btheta)}{p(\btheta|\bx)} =& \int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\btheta|\bx)\right)d\btheta\\
=&\int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\btheta, \bx) - \log p(\bx)\right)d\btheta\\
=&\underbrace{- \log p(\bx)}_{\leq 0} + \int q_{\bphi}(\btheta) \left(\log q_{\bphi}(\btheta) - \log p(\bx|\btheta)- \log p(\btheta) \right)d\btheta\\
\leq& -\expec{q_{\bphi}}{\log p(\bx|\theta)} + \KL{q_{\bphi}(\btheta)}{p(\btheta)} = \VFE(\bphi)
\end{align}


By minimizing the \ac{VFE}: $\VFE(\bphi)$ instead of the \ac{KL} divergence, we expect to find a solution close to the optimum of the problem stated in \eqref{eq:prob_VI}.
A standard way is to perform gradient descent on the variational parameters $\bphi$
\begin{align}
\bphi^{t+1} = \bphi^{t} - \epsilon \grad_{\bphi}\VFE(\bphi^{t}).
\end{align}

Computing the gradient $\grad_{\bphi}\VFE(\bphi)$ can be non-trivial, but many methods were developed to tackle this problem.

\subsubsection{Mean-Field Approximation}

One of the most important additions to the \ac{VI} method is the \ac{MF} approximation.
\ac{MF} is the assumption that the variational distribution $q(\btheta)$ assumes every component of $\btheta$ to be independent of each other.
This variational distribution can be specified as:
\begin{align}
q^{MF}_{\bphi}(\btheta) = \prod_{i=1}^D q_{\bphi_i}(\theta_i),
\end{align}
where $\bphi_i$ are the variational parameters for the variable $\theta_i$.

One can also build more general distributions by considering independence between blocks of variables instead.
We can split $\mathcal{I}=\{1,2,\ldots,D\}$, the set of indices of $\theta$, into $K$ independent subsets $\mathcal{I}_k \subseteq \mathcal{I}$ such that  $\mathcal{I} = \cup_{k=1}^K \mathcal{I}_{k}$ and $\mathcal{I}_i \cap \mathcal{I}_j=\emptyset,~\mathrm{iff}~i \neq j$.
The variational distribution based on this blocked mean-field approximation is then defined as
\begin{align}
    q^{BMF}_{\bphi}(\btheta) = \prod_{k=1}^K q_{\bphi_k}(\btheta_{\mathcal{I}_k}),
\end{align}
where $\bphi_k$ are the variational parameters for the set of variable $\theta_{\mathcal{I}_k}$.
\subsubsection{Coordinate Ascent VI}

Following the \ac{MF} approach, it is sometimes possible to find the optimal parameters $\bphi^*$ in closed-form.
By solving:
\begin{align}
\left.\grad_{\varphi_i}\VFE(\bphi)\right\vert_{\varphi_i=\varphi_i^*} = 0,
\end{align}
for each variable $\varphi_i$ we can find a local optima.
The advantage of this method is that one can also perform it independently for each latent variable $\theta_i$.
Concretely the updates are of the form:
\begin{align}
q_{\varphi_i}^*(\theta_i) \propto \exp\left(\expec{q_{\bphi}(\btheta_{/i})}{\log p\left(\theta_i|\btheta_{/i},\bx\right)}\right)
\end{align}
where $\btheta_{/i}$ represent the collection of variables $\btheta_{/i} = \{\theta_j | j \neq i\}$.
When working with distribution coming from exponential families, it is straightforward to get the optimal variational parameters $\varphi_i$.
By updating the parameters one after another we get a \ac{CAVI} scheme\footnote{The word ascent is used since the scheme was originally derived using the negative \ac{VFE} i.e. the \ac{ELBO}.}.
Effectively, one update each variational parameter $\varphi_i$ by its optimum given the rest of the variational parameters $\bphi_{/i}$ via closed-form functions:
\begin{align}
\varphi_i^{t+1} = f_i\left(\bphi_{1:(i-1)}^{t+1}, \bphi_{(i+1):D}^t\right).
\end{align}
The order of the updates do not matter as long as the variational parameters $\bphi$ are initialized in their domain.

The algorithm is presented completely on Algorithm~\ref{alg:CAVI}.

\begin{algorithm}
    \caption{\ac{CAVI} Updates}
    \label{alg:CAVI}
\end{algorithm}


\subsubsection{Natural Gradients}

One interesting aspect of \ac{CAVI}, is that it implicitly brings the concept of "Natural Gradients" \cite{amariNaturalGradientWorks1998}.
A natural gradient is a gradient with the inverse Fisher matrix applied as a preconditioner.
The Fisher information matrix is defined as 
\begin{align*}
    \mathcal{I}_\theta = \expec{p(\bx|\btheta)}{\left(\grad_{\btheta}\log p(\bx|\btheta)\right)\left(\grad_{\btheta} \log p(\bx|\btheta)\right)^\top} = -\expec{p(\bx|\btheta)}{\mathbf{H}(\log p(\bx|\btheta))},
\end{align*}
where $\mathbf{H}(f)$ is the Hessian matrix of the function $f$.
\com{Give more details about the meaning of the Fisher info}
The natural gradient is therefore given by :
\begin{align*}
    \widetilde{\grad}_{\bphi}\mathcal{F}(\bphi) = \mathcal{I}^{-1} \grad_{\bphi}\mathcal{F}(\bphi)
\end{align*}
This preconditioning of the gradients move the gradient descent in a different Riemannian metric.
The natural gradient is constructed such that the metric it gives maximizes the change of the \ac{KL} divergence between the given distribution and its target.
The reason why natural gradients are brought up here is that the updates of the \ac{CAVI} algorithm \ref{alg:CAVI} for exponential distributions, can be interpreted as natural gradient ascent updates with learning rate $1$.
\begin{align*}
    \phi^{t+1} = \phi^t + \mathcal{I}^{-1}\grad_{\phi}\mathcal{F}(\bphi^t) \equiv \phi^{t+1} = 
\end{align*}



% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------