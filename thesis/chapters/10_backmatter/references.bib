
%% Created for Jakob Suckale at 2007-09-06 11:30:44 +0200 


%% Saved with string encoding Unicode (UTF-8) 

@article{Titsias2009,
  title        = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}},
  author       = {Titsias, Michalis},
  date         = {2009},
  journaltitle = {Aistats},
  volume       = {5},
  pages        = {567--574},
  issn         = {15324435},
  url          = {http://eprints.pascal-network.org/archive/00006353/},
  abstract     = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  keywords     = {Learning/Statistics & Optimisation,Theory & Algorithms},
  annotation   = {ZSCC: 0000724},
  file         = {/home/theo/Zotero/storage/CYDPNW5X/Variational Learning of Inducing Variables in Sparse Gaussian Processes - Titsias - 2009.pdf}
}


