% -*- root: ../thesis.tex -*-
%!TEX root = ../thesis.tex
% ******************************* Thesis Chapter 7 ****************************

% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\graphicspath{{9_conclusion/figures/}}
% ----------------------- contents from here ------------------------With this thesis, I want to motivate the use of different representations to ease inference in probabilistic models.
The work on scale mixtures proves to be very efficient and exploits the best out of the blocked Gibbs sampling and the blocked \ac{CAVI} algorithms.
The steps of these algorithms are typically complicated to derive, but with the augmentations, we can derive them with ease.

However, we do not have a clear theoretical understanding of the reason for this efficiency.
By exploring the properties of these likelihoods, we work on obtaining bounds on the convergence speed of these algorithms.
An intuition on why these augmentations work so well is the notion of decoupling.
Many inference bottlenecks come from very highly-correlated variables and heavy tails.
By separating these components into different variables, all parts become easier to model and do not suffer from the typical inference issues mentioned beforehand.
These ideas do not represent an actual theory for now, and we need a thorough analysis.
A better understanding could give insights into how convergence speed and variable correlation are connected.

Another challenge, as pointed out in Chapter~\ref{ch:discussion}, is to widen the class of functions viewed as mixtures.
The most promising lead is moment generating functions, but there is little theory on their properties.
\citet{schwartz1952transformation} is one of the few persons who developed a theory on distributions based on their Laplace distribution. However, these texts can be confusing as distributions can have different meanings (analytic or probabilistic).

Regardless, one of the biggest challenges is to popularize the use of such models.
The approach of \citet{Hensman2015} is by far the most popular, partly due to the success of the \href{https://github.com/GPflow/GPflow}{GPFlow} library \cite{GPflow2017}.
Implementing these augmentations in popular libraries would be a good step.
There have been an effort in Julia with the \href{https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl}{AugmentedGPLikelihoods.jl} \cite{theo_galy_fajou_2022_6347022}, and implementations in \href{https://gpytorch.ai/}{GPyTorch} \cite{gardner2018gpytorch} or GPFlow would accelerate the adoption of these techniques.



% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
