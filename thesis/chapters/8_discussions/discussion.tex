% -*- root: ../thesis.tex -*-
%!TEX root = ../thesis.tex
% ******************************* Thesis Chapter 7 ****************************

% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\graphicspath{{8/figures/}}
% ----------------------- contents from here ------------------------


\section{Further generalizations and understanding}
The works presented in this thesis only scratched the surface of how mixtures can be helpful and how it is possible to identify them.
We are still exploring ways to identify larger classes of functions which can be evaluated as scale-mixtures.
In particular, the connection with the \acf{MGF} of a distribution is a promising direction.
It allows working not only with continuous but discrete variables.
Examples of this are two of the augmentations of Chapter~\ref{ch:multiclass} which do not fit the framework introduced in Chapter~\ref{ch:general}.
The \ac{MGF} is also an interesting tool to create hierarchical models since the mixture will be of the form $\sum_{i=0}^\infty e^{tx} p(x)$ or $\int e^{tx}p(x)dx$.
From there, we can plug in P\'olya-Gamma augmentations, using the transformation $t=\log \sigma(x)$, in particular using the property that not only $\sigma(x)$ is augmentable but also $\sigma^n(x),~\forall n\in \mathbb{R}^+$.
Additional examples of such constructions are shown in this chapter in Sections \ref{sec:heteroscedastic} and \ref{sec:improvemulticlass}.

Another promising improvement in terms of inference is to identify when some augmented variables can be marginalized out while keeping the conditional conjugacy of the model.
For example, in the multi-class model, one random variable can be marginalized out as shown in Section~\ref{sec:improvemulticlass}.
This marginalization step can be avoided altogether by identifying from the start the right \ac{MGF}, but since the marginalized distribution can sometimes be some less known function this additional step can be beneficial. 

Finally, an incomplete part of this work was to establish convergence rates (error as a function of the number of iterations) for the \ac{CAVI} algorithm and of the correlation of the kernel for the Gibbs sampler.
Experimental results seem to indicate that the error is going down as $\|\mathcal{L}^{*} - \mathcal{L}^{t}\|\leq C e^{-t}$, where $t$ is the number of iterations, but no formal proof could be made in this regard.

\section{Double bounds for intricate latent GPs}
\label{sec:heteroscedastic}
The model developed in Chapter~\ref{ch:multiclass} paves the ways to work with multi-latent models and hierarchical augmentations.
Another model explored, but not published, is the heteroscedastic Gaussian regression model which was already explored in \cite{wangGaussianProcessRegression2012}.
By having two latent \ac{GPs} $f$ and $g$, one can model simultaneously the mean and the variance of the likelihood.
An important choice is the link between $g$ and the likelihood variance $\epsilon^2$ ensuring positivity.
The exponential link, i.e. $\epsilon^2(x) = \exp(g(x))$, is the most popular, however to be able to apply our augmentations, we use the link $\epsilon^2(x) = \left(\lambda \sigma(g(x))\right)^{-1}$.
The likelihood becomes :
\begin{align}
    p(y|f,g,\lambda) = \frac{\sqrt{\lambda \sigma(g)}}{\sqrt{2\pi}}\exp\left(-\frac{\lambda \sigma(g)(y-f)^2}{2}\right).\label{eq:hetero_lik}
\end{align}

The augmentations are straightforward and quite similar to the multi-class ones from Chapter~\ref{ch:multiclass}.
\begin{align*}
    \exp\left(-\frac{\lambda\sigma(g)(y-f)^2}{2}\right) =& \exp\left(\frac{\lambda(\sigma(-g) -1)(y-f)^2}{2}\right)\\
    =&\sum_{n=0}^\infty \sigma^n(-g)\Po \left(n~\middle|~\frac{\lambda(y-f)^2}{2}\right),
\end{align*}
where we used the \ac{MGF} of the Poisson distribution.
Now using the P\'olya-Gamma augmentation and the additivity property of P\'olya-Gamma variables, we get the final augmented likelihood:
\begin{align}
    p(y,n,\omega\mid f,g,\lambda) = \frac{\sqrt{\lambda}}{2^n\sqrt{\pi}}\exp\left(\frac{1}{2}\left(g\left(\frac{1}{2}-n\right) - \frac{g^2}{\omega}\right)\right)\PG \left(\omega \mid \half + n, 0\right) \Po \left(n \mid \lambda\frac{(y-f)^2}{2}\right)\label{eq:heteroscedastic}
\end{align}
The interesting part about this augmented likelihood is that although it is conditionally conjugate in $g$, $\omega$ and $n$, it is not clear how inference works for $f$.
It turns out that the Gibbs sampler for this model is actually very simple: for the full-conditional of $f$, we can simply marginalize $n$ and $\omega$, return to the original likelihood, which is conditionally conjugate, and perform a \textbf{collapsed step} for the Gibbs sampler.

However, as underlined in Section~\ref{sec:cavi}, the \ac{CAVI} updates need the full-conditionals of the model, and the marginalized version is not guaranteed to work.
To solve this problem we define an additional bound on the original variational bound:
\begin{align*}
    \KL{q(\boldf)q(\boldg)}{p(\boldf,\boldg|\boldy)}\leq& \min_{q(\boldg)}-\expec{q(\boldg)}{\expec{q(\boldf)}{\log p(\boldy|\boldf,\boldg)}} + \KL{q(\boldf)q(\boldg)}{p(\boldf)p(\boldg)}\\
    =& \min_{q(\boldg)}-\expec{q(\boldg)}{\log p(\boldy|\boldg, \bmu^*_f,\bSigma^*_f)} + \KL{q(\boldg)}{p(\boldg)} + \operatorname{KL}_f^* = \mathcal{F}_1.
\end{align*}
$p(\boldy|\boldg, \bmu^*_f, \bSigma^*_f)$ and $\mathrm{KL}^*_f$ are expectations computed with the optimal $q^*(\boldf)$.
We can now use the augmentations from equation~\ref{eq:heteroscedastic} on the expected log-likelihood, where we replaced $(y_i-f_i)^2$ by $(y_i - \mu_i^2)^2 + \Sigma_{ii}$.

\begin{align*}
    \mathcal{F}_1 \leq \min_{q(\boldg)q(\bomega,\bn)} \expec{q(\boldg)q(\bomega,\bn)}{\log p(\bomega,\bn,\boldy|\boldg,\bmu_f^*,\bSigma_f^*)} + \KL{q(\boldg)}{p(\boldg)} + \operatorname{KL}_f^*=\mathcal{F}_2
\end{align*}

It is straightforward to find the optimal variational distributions $q^*(\boldg)$ and $q^*(\bomega,\bn)$ minimizing $\mathcal{F}_2$ which allows us to use \ac{CAVI} updates.
Then, injecting the optimal distributions we can derive the optimal $\bmu_f^*$ and $\bSigma_f^*$, which once again, are obtainable in closed-form.


\section{Using Hamilton Monte Carlo on the augmented model}

The Gibbs sampler in the experiments of Chapters~\ref{ch:classification} and \ref{ch:general} outperforms the state-of-the-art \ac{HMC} algorithm introduced in Section~\ref{sec:hmc}.
A natural question to ask is: is the performance gain only due to the augmentation or also connected to the use of a Gibbs sampler?
A first answer to this is that the \ac{HMC} sampler is not well adapted to the augmentation due to the large increase of dimensionality of the model.
For $N$ observations, it adds at least $N$ more dimensions, making gradients computations and the algorithm parameters tuning more complicated and expensive.
The second issue with \ac{HMC} with the augmented model is that the \ac{pdf} of the augmented variables prior is not always available in closed-form.
For example, to estimate the probability of a P\'olya-Gamma variables, a truncated alternating series is used, which could make the computations biased and even sometimes unstable!
This is not only limited in accuracy but also much more expensive to compute.
But if one perseveres, the Figure~\ref{fig:hmc_vs_gibbs} shows the auto-correlation plots on \ac{GP} regression problem with a Student-t likelihood with 3 degrees of freedom applied on the Boston housing dataset (506 data points, 13 dimensions) \cite{harrison1978hedonic}.
One chain of 2000 samples (plus 500 adaption  samples for \ac{HMC} and \ac{NUTS}) is drawn.

From the first look, \ac{HMC} applied on the augmented model seems to indeed have a lower auto-correlation.
When using \ac{NUTS}, the gain becomes less clear, moreover the algorithm produces antithetic chains which makes it harder to analyze.
The Gibbs sampler is having the least intra-chain correlation, but one could argue that negative correlations are more desirable.
However, \ac{HMC} and \ac{NUTS} turned out to be much slower than the Gibbs sampler:
the Gibbs sampler took around 20 sec to run against an average of 12 minutes for \ac{HMC} and \ac{NUTS}.
Perhaps surprisingly, there was not a big difference of time between the augmented and original models.

This should of course be considered only as a preliminary results, as a simple likelihood was used and the dataset should be considered trivial.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{./chapters/8_discussions/figures/autocorrelation.pdf}
    \caption{Auto-correlation function of the Gibbs sampler, \ac{HMC} and \ac{NUTS} on the augmented model, and \ac{HMC} and \ac{NUTS} on the original model.
    The mean is shown with one standard-deviation over all dimensions.}
    \label{fig:hmc_vs_gibbs}
\end{figure}


\section{Improvements on the Multi-Class Classification}
\label{sec:improvemulticlass}
We recently figured that there were additional ways to improve the augmented models and the associated inference.

\subsection{Simplex for the multi-class classification}
\label{sec:simplex}
In Chapter \ref{ch:multiclass}, one of the possible concerns regarding the model is that $\lambda$ has the improper prior $p(\lambda) = 1_{[0,\infty)}$, which is a proper measure but is not normalizable.
It should be noted that since the resulting posterior is valid, no problem should arise, but from a purely Bayesian perspective this can be dissatisfactory.
This improper prior rises from the fact that the model we are using is ill-conditioned.
Given $K$ classes, only $K-1$ latent \ac{GP} are needed since we have the additional constraint that $\sum_{k=1}^K p(y=k|\boldf) = 1$.
Using this formulation we get the following likelihood:
\begin{align}
    p(y=k|\{f_k\}_{k=1}^{K-1}) = \left\{
        \begin{array}{cc}
            \frac{\sigma(f_k)}{D + \sum_{i=1}^{K-1}\sigma(f_i)}, & \mathrm{if}\; 1 \leq k < K - 1\\
            \frac{D}{D + \sum_{i=1}^{K-1}\sigma(f_i)}, & \mathrm{if}\; k = K - 1 \\
    \end{array}
    \right.,
\end{align}
where $D \in [0, 1]$.

Using this formulation, the first augmentation that led to an improper prior :
\begin{align*}
    \frac{1}{ \sum_{i=1}^{K} \sigma(f_i)} = \int_0^\infty e^{-\lambda  \sum_{i=1}^{K} \sigma(f_i)}d\lambda
\end{align*}
becomes the known \ac{MGF} of a Gamma distribution with a proper mixture:
\begin{align*}
    \frac{1}{D + \sum_{i=1}^{K-1} \sigma(f_i)} =& \frac{1}{D + K - 1 - \sum_{i=1}^{K-1} \sigma(-f_i)}\\
    =& \frac{1}{D + K - 1}\int_0^\infty e^{-\lambda \sum_{i=1}^{K-1} \sigma(-f_i)}\Ga \left(1, \frac{1}{D + K - 1}\right)d\lambda
\end{align*}
Note that an important condition to use the \ac{MGF} is that $\sum_{i=1}^{K-1} \sigma(-f_i) < D + K - 1$ which is true as long as $D > 0$.
This condition would not necessarily hold if we used the over-parametrized version which would lead to $\sum_{i=1}^{K} \sigma(-f_i) < K$ (not true when taking the limits $\forall i,\;f_i \rightarrow -\infty$).

From there the derivations are the same, i.e. using the \ac{MGF} of a Poisson distribution and finally using the P\'olya-Gamma augmentation.
Unfortunately we did not run any experiments to explore if one parametrization was more accurate or efficient than another.

\subsection{Marginalizing out variables}
In the augmentation derived in Chapter~\ref{ch:multiclass}, we add $2K + 1$ new variables per observation: $\lambda$, $\{n_i\}_{i=1}^K$ and $\{\omega_i\}_{i=1}^K$.
However, we can reduce this number to $2K$ by marginalizing out one of the variables and therefore avoiding complicated inner loops.
When deriving the augmentations, at some point, one ends up with the following augmented likelihood:
\begin{align}
    p(y=k|\{f_i\}_{i=1}^K) = \sigma(f_k) \prod_{i=1}^K \sigma(-f_i)^{n_i} \Po (n_i|\lambda).
\end{align}
We can marginalize out the $\lambda$:
\begin{align}
    \int_{0}^\infty \prod_{i=1}^K \sigma(-f_i)^{n_i} \Po (n_i|\lambda)d\lambda =& \frac{1}{\prod_{i=1}^K n_i!}\int_{0}^\infty \lambda^{\sum_{i=1}^K n_i}e^{-K\lambda}d\lambda \nonumber\\
    =& \frac{K^{-\sum_{i=1}^K n_i}}{\prod_{i=1}^K n_i!}\prod_{i=1}^K \sigma(-f_i)^{n_i} \int_{0}^\infty (K\lambda)^{\sum_{i=1}^K \sigma(f_i)}e^{-K\lambda}d\lambda \nonumber\\
    =& \Gamma(1 + \sum_{i=1}^K n_i) \prod_{i=1}^K \left(\frac{\sigma(-f_i)}{K}\right)^{n_i}\frac{1}{n_i!} \label{eq:NM}
\end{align}

Which is proportional to a negative multinomial $\operatorname{NM}(x_0, \boldsymbol{p})$ with parameters $x_0=1$, $\boldsymbol{p}=\left\{\frac{\sigma(-f_i)}{K}\right\}_{i=1}^K$:
\begin{align*}
    \operatorname{NM}(\bx|x_0,\boldsymbol{p}) = \Gamma\left(\sum_{i=0}^K x_i\right) \frac{p_0^{x_0}}{\Gamma(x_0)}\prod_{i=1}^K \frac{p_i^{x_i}}{x_i!}   
\end{align*}
where $p_0 = 1 - \sum_{i=1}^K p_i$.
Note that $p_0$ is the missing normalization constant in Equation~\ref{eq:NM} and that it would directly depend on $\{f_i\}_{i=1}^K$, making the likelihood non-normalizable.
The effect mentioned in Section~\ref{sec:simplex} is still propagated to the marginalized augmented likelihood.

More interestingly, all these derivations could have been avoided by noticing that the \ac{MGF} of a negative binomial distribution is given by:
\begin{align*}
    \operatorname{MGF}_{\operatorname{NM}(x_0,\boldsymbol{p})}(\boldsymbol{t}) = \left(\frac{p_0}{1-\sum_{i=1}^K p_k e^{t_i}}\right)^{x_0},
\end{align*}
This makes it even easier to derive the correctly parametrized model proposed in Section~\ref{sec:simplex}.
Quickly identifying the terms in the \ac{MGF}, the augmented likelihood would be:
\begin{align*}
    p\left(y=k,\{n_i\}_{i=1}^{K-1}|\{f_i\}_{i=1}^{K-1}\right) = \frac{\sigma(f_k)}{D}\left(\prod_{i=1}^{K-1}\sigma(-f_i)^{n_i}\right)\operatorname{NM}\left(\boldsymbol{n}\mid 1, \left\{\frac{1}{D+K-1}\right\}_{i=1}^{K-1}\right),
\end{align*}
where we set $\sigma(f_K) = D$.

\subsection{Reformulating the logistic-softmax link}

Another issue that rose from this paper was the logistic-softmax link limitations, especially in the many-class problem.
Because of the bounds of the logistic function, extremely high value of $f_i$ are needed to reach class prediction probability of $1$.
This could be easily solved by using a scaled logistic function instead.
One could add $K$ hyperparameters $\btheta$ such that the likelihood is written as:
\begin{align*}
    p(y=k|\{f_i\}_{i=1}^K,\btheta) = \frac{\theta_k \sigma(f_k)}{\sum_{i=1}^K \theta_i \sigma(f_i)}.
\end{align*}
The $\btheta$ parameters can be optimized using the \ac{ELBO} as the other hyperparameters.
These can also provide some information about each class, a high $\theta_i$ means that the $i$-th class has zones of very high confidence.

\section{Sampling from a sparse augmented model}

Another work in progress regards the sampling of sparse \ac{GPs} models.
Being able to sample directly from the augmented model is ideal as it allows recovering samples from the original posterior $p(\boldf|\bX,\boldy)$.
Unfortunately, this does not transfer when using inducing points.
Using inducing points naively, without using the Titsias assumption \cite{Titsias2009}, leads to an algorithm with complexity $\mathcal{O}((N+M)^3)$, defeating the point of the algorithm.
Instead, one can resort to mix a variational approach to sampling.
By considering a free-form variational distribution $q(\boldu)$ defined as a collection of samples one can solve the problem as in \citet{hensmanMCMCVariationallySparse2015}.
In other words, we want to sample from the optimal $q^*(\boldu)$ defined as:
\begin{align*}
    \log q^*(\boldu) = \expec{p(\boldf|\boldu)}{\log p(\boldy|\boldf)} + \log p(\boldu) + C.
\end{align*}
The straightforward option is to use \ac{HMC} as in \cite{hensmanMCMCVariationallySparse2015}, but we can also derive a Gibbs sampling algorithm to sample from this objective using our previous work.

In our new setting the optimal distribution is defined as $q(\boldf,\boldu,\bOmega) = p(\boldf|\boldu)q(\boldu, \bOmega)$, where $\bOmega$ denotes the set of augmented variables.
Note, that unlike in the traditional \ac{VI} setting, $\boldu$ and $\bOmega$ are not assumed to be independent.
From there, $\bOmega$ and $\boldu$ are sampled one after another using the optimal distributions $q^*(\bOmega|\boldu,\boldf)$ and $q^*(\boldu,\boldf|\bOmega)$.
Note that $\boldu$ and $\boldf$ are sampled together since they are highly correlated.
To identify the optimal conditional distributions, we look at the minimizer of the conditional \ac{KL} divergence, e.g. $q^*(\bOmega|\boldu,\boldf) = \arg_{q} \min \KL{q(\bOmega|\boldu,\boldf)}{p(\bOmega|\boldu,\boldf,\boldy,\bX)}$.
The obvious minimizer here is to set $q^*=p$, and thanks to the augmentations, we know $p(\bOmega|\ldots)$ in closed-form.
We can perform a similar approach for $q^*(\boldu,\boldf|\bOmega)$.
The only parameters left are the hyperparameters (omitted in the previous equations), which can be sampled by using the more traditional \ac{HMC} algorithm, in a separate Gibbs sampling step.
The advantage of this approach is that no gradients are necessary for the variational parameters as all full-conditionals are known in closed-form and that no expectation over the log-likelihood are needed at all.
This opens up more possibilities over more complex problems like the multi-class or heteroscedastic one.
For medium-sized datasets this can even prove to outperform the \ac{CAVI} algorithm as it has the same convergence speed but does not suffer from the mean-field assumption on the variational parameters.
Hopefully, this work should be released in the near future.

\section{Limits}

As pointed out in chapters about augmentations, working with augmented models has its own issues.
The main one being of course having to identify potential for augmentations in a given model.
The idea generalization of using \ac{MGF} is promising, but is also limited.
Often terms need to be rearranged, identities need to be used to reach something usable.
This is feasible for someone with expertise, but it is a very complicated problem to automatize the process, in the context of probabilistic programming for example.

Another issue is the variational distribution $q(\boldf, \bOmega)$ optimized on the augmented model is not as accurate as the variational distribution $q(\boldf)$ optimized on the original model.
Interestingly the difference of bound comes exclusively from the mean-field assumption between $q(\boldf)$ and $q(\bOmega)$.
Since the predictive results prove to be almost as good and that the difference of bounds is not always significant, this gives us a strong sign that the two set of variables are naturally strongly decorrelated and this would give a strong clue on why the Gibbs sampling and \ac{CAVI} updates tend to be so efficient.
In fact, the work of \citet{gorinovaAutomaticReparameterisationProbabilistic2020} mentioned in the introduction aims at finding a perfect balance between two parametrizations by ensuring that the posterior is as close as possible to a multivariate Gaussian with independent dimensions.




% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
