% -*- root: ../thesis.tex -*-
%!TEX root = ../thesis.tex
% ******************************* Thesis Chapter 7 ****************************

% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\graphicspath{{8/figures/}}
% ----------------------- contents from here ------------------------
This chapter presents both discussions and extensions on the models and ideas presented in Chapters~\ref{ch:classification},~\ref{ch:multi-class},~\ref{ch:general}.
Section~\ref{sec:further} considers how augmentations can be generalized further and what analysis we need to fully understand the improvement brought by augmentations.

\section{Further generalizations and understanding}
\label{sec:further}
The works presented in this thesis only scratched the surface of how helpful mixtures and representations are.
We are still exploring ways to identify larger classes of functions identifiable as scale mixtures or hierarchical mixtures.
Already mentioned in Chapters~\ref{ch:multi-class} and \ref{ch:general},
the connection with the \acf{MGF} of a distribution is a promising direction.
\ac{MGF} allow us to work not only with continuous but also discrete and multivariate variables.
Even, if the augmented variable does not come from a known distribution, we know how to sample from it \cite{ridout2009generating}.
For example, the Poisson augmentation of Chapter~\ref{ch:multi-class} does not fit the framework introduced in Chapter~\ref{ch:general}.
We show in Section~\ref{sec:improvemulticlass}, that this transformation is more simply understood through the lens of \ac{MGF}s.
The \ac{MGF} is also an interesting tool to create hierarchical models since scale mixtures are of the form $\sum_{x} e^{tx} p(x)$ or $\int e^{tx}p(x)dx$.
From there, given $t=\log \sigma(f)$, we can use P\'olya-Gamma augmentations, thanks to the property that not only $\sigma(f)$ is augmentable but also $\sigma^n(f)$, for any $n\in \mathbb{R}^+$.
Additional examples of such constructions are shown in this chapter in Sections \ref{sec:heteroscedastic} and \ref{sec:improvemulticlass}.

A potential improvement for augmented models is the identification of marginalizable augmented variables that keep the conditional conjugacy of the model.
For example, in the multi-class model from Chapter~\ref{ch:multi-class}, the augmented variable $\lambda$ can be marginalized out, as shown in Section~\ref{sec:improvemulticlass}.
We can reduce the dimensionality of the model and avoid tricky situations like the inner loop updates in Chapter~\ref{ch:multi-class}.
This marginalization step is avoidable by identifying the right \ac{MGF} from the start.
As shown in Section~\ref{sec:heteroscedastic_nongaussian}, switching between marginalized and augmented models gives a great inference flexibility.

Finally, an unfinished work (despite trying) is to establish convergence rates (error as a function of the number of iterations) for the \ac{CAVI} algorithm and of the correlation of the kernel for the Gibbs sampler.
Experimental results indicate that the error on the variational free energy is decreasing as $\|\VFE^{*} - \VFE^{t}\|\leq C e^{-t}$, where $t$ is the number of iterations, but we did not manage to write a formal proof.


\section{Double bounds for intricate latent GPs}
\label{sec:heteroscedastic}
The multi-class model developed in Chapter~\ref{ch:multi-class} paves the way to work with multi-latent models and hierarchical augmentations.
Based on this idea, we developed another multi-latent model on the heteroscedastic regression likelihood  \cite{wangGaussianProcessRegression2012,lazaro2011variational}.
It models simultaneously the mean and variance of a regression likelihood with two latent \ac{GPs} $f$ and $g$.
We consider both Gaussian and Non-Gaussian likelihoods since augmentations can stack in this situation.
We start with the simplest model: the heteroscedastic Gaussian likelihood.

\subsection{Heteroscedastic Gaussian Likelihood}
\label{sec:hetero_gaussian}
A crucial model choice is the function mapping $g$ to the likelihood variance $\epsilon^2$.
The exponential link, i.e. $\epsilon^2(x) = \exp(g(x))$, is the most popular, however to be able to apply our augmentations, we use the link $\epsilon^2(x) = \left(\lambda \sigma(g(x))\right)^{-1}$.
Let's look at the case of the heteroscedastic Gaussian likelihood, defined as:
\begin{align}
    p(y|f,g,\lambda) = \frac{\sqrt{\lambda \sigma(g)}}{\sqrt{2\pi}}\exp\left(-\frac{\lambda \sigma(g)(y-f)^2}{2}\right).\label{eq:hetero_lik}
\end{align}

The augmentations for this likelihood are straightforward and quite similar to the multi-class ones from Chapter~\ref{ch:multi-class}.
\begin{align}
    \exp\left(-\frac{\lambda\sigma(g)(y-f)^2}{2}\right) =& \exp\left(\frac{\lambda(\sigma(-g) -1)(y-f)^2}{2}\right)\nonumber\\
    =&\sum_{n=0}^\infty \sigma^n(-g)\Po \left(n~\middle|~\frac{\lambda(y-f)^2}{2}\right),\label{eq:aug_poisson}
\end{align}
where we used the \ac{MGF} of the Poisson distribution.
Using the P\'olya-Gamma augmentation and the additivity property of P\'olya-Gamma variables, we get the final augmented likelihood:
\begin{align}
    p(y,n,\omega\mid f,g,\lambda) = \frac{\sqrt{\lambda}}{2^n\sqrt{\pi}}\exp\left(\frac{1}{2}\left(g\left(\frac{1}{2}-n\right) - \frac{g^2}{\omega}\right)\right)\PG \left(\omega \mid \half + n, 0\right) \Po \left(n \mid \lambda\frac{(y-f)^2}{2}\right)\label{eq:heteroscedastic}
\end{align}
The interesting part about this augmented likelihood is that although it is conditionally conjugate in $g$, $\omega$ and, $n$, it is unclear how to infer $f$:
the final likelihood~\eqref{eq:heteroscedastic} is quadratic in $g$ but not in $f$.
It turns out that the Gibbs sampler for this model is very simple:
We take the augmented likelihood $p(y|f,g,\lambda,\omega,n)$, we marginalize out $n$ and $\omega$ and, as expected, we get the original likelihood~\eqref{eq:heteroscedastic}, which is conditionally conjugate with $f$.
The conditional $p(f|y,g,\lambda)$ on this likelihood is the \textbf{collapsed conditional}.
In a Gibbs sampling scheme, this allows us to perform a \textbf{collapsed step}.
We give all the Gibbs sampling steps in Algorithm~\ref{alg:gibbs_hetero}.
So far, we have excluded the $\lambda$ parameter from inference.
By putting a Gamma prior $\Ga(\lambda|\alpha, \beta)$, where $\alpha$ is the shape and $\beta$ is the rate, the collapsed conditional is available in closed-form:
\begin{align*}
    p(\lambda|\boldf, \boldf, \boldy) = \Ga(\lambda| \alpha + \frac{N}{2}, \beta + \sum_{i=1}^N \frac{\sigma(g^i)}{2}(y^i - f^i)^2).
\end{align*}


As underlined in Section~\ref{sec:cavi}, the \ac{CAVI} updates need the full-conditionals of the model, and is not compatible with collapsed conditionals.
To solve this problem, we need to reverse-engineer how \ac{CAVI} updates are obtained and start with a first bound on the \ac{KL} divergence:
\begin{align*}
    \KL{q(\boldf)q(\boldg)}{p(\boldf,\boldg|\boldy)}\leq& \min_{q(\boldg)}-\expec{q(\boldg)}{\expec{q(\boldf)}{\log p(\boldy|\boldf,\boldg)}} + \KL{q(\boldf)q(\boldg)}{p(\boldf)p(\boldg)} - \log p(\boldy)\\
    =& \min_{q(\boldg)}-\expec{q(\boldg)}{\log p(\boldy|\boldg, \bmu^*_f,\bSigma^*_f)} + \KL{q(\boldg)}{p(\boldg)} + \operatorname{KL}_f^* - \log p(\boldy)= \mathcal{F}_1.
\end{align*}
$p(\boldy|\boldg, \bmu^*_f, \bSigma^*_f)$ and $\mathrm{KL}^*_f$ are expectations computed with the optimal $q^*(\boldf)=\mathcal{N}\left(\boldf|\bmu_f^*,\bSigma_f^*\right)$.
We can now use the augmentations from Equation~\eqref{eq:heteroscedastic} on the expected log-likelihood, where we replaced $(y_i-f_i)^2$ by $(y_i - (\bmu_f^*)_i)^2 + (\Sigma_f^*)_{ii}$, and build a second bound.

\begin{align}
    \mathcal{F}_1 \leq \min_{q(\boldg)q(\bomega,\bn)} \expec{q(\boldg)q(\bomega,\bn)}{\log p(\bomega,\bn,\boldy|\boldg,\bmu_f^*,\bSigma_f^*)} + \KL{q(\boldg)}{p(\boldg)} + \operatorname{KL}_f^*=\mathcal{F}_2 \label{eq:second_bound}
\end{align}

It is straightforward to find the optimal variational distributions $q^*(\boldg)$ and $q^*(\bomega,\bn)$ minimizing $\mathcal{F}_2$ which allows us to use \ac{CAVI} updates.
Then, injecting the optimal distribution $q^*(\boldg)q(\bomega,\bn)$ in $\mathcal{F}_2$, we can derive the optimal $\bmu_f^*$ and $\bSigma_f^*$, obtainable in closed-form.
The resulting \ac{CAVI} updates are given in Algorithm~\ref{alg:cavi_hetero}.
For $\lambda$, we can use the second bound \eqref{eq:second_bound} and obtain a closed-form maximum-likelihood estimate, given in Algorithm~\ref{alg:cavi_hetero}.

This double-bound approach is very similar to \citet{lazaro2011variational}, although they are using the exponential link and need some extra computations.

\begin{algorithm}[H]
    \caption{Gibbs sampling for the Heteroscedastic Gaussian likelihood}
    \begin{algorithmic}
        \State \textbf{input:} $\boldf,\boldg, \lambda, \boldy$, $p(\boldf,\boldg) = \mathcal{N}(\boldf|\bmu^0_f, K)\mathcal{N}(\boldg|\bmu^0_g, K)$, $p(\lambda|\alpha,\beta)$.
        \For{$t$ in $1:$ N samples}
            \State Draw $\lambda \sim p(\lambda|\boldf,\boldg,\boldy) = \Ga(\lambda| \alpha + \frac{N}{2}, \beta + \sum_{i=1}^N \frac{\sigma(g^i)}{2}(y^i - f^i)^2).$
            \State Draw $n^i \sim p(n^i|f^i, g^i, \lambda) = \Po(\lambda \sigma(-g^i) \frac{(y^i - f^i)^2}{2})$
            \State Draw $\omega^i \sim p(\omega^i|n^i,g^i) = \PG(0.5 + n^i, |g^i)$
            \State Draw $\boldg \sim p(\boldg|\boldn, \bomega) = \mathcal{N}(\bmu_g,\bSigma_g)$
            \State \quad where $\bSigma_g=  \left(K^{-1} + \diag(\bomega)\right)^{-1}$ and $\bmu_g = \bSigma_g\left(K^{-1}\bmu^0_g + \frac{0.5 - \boldn}{2}\right)$
            \State Draw $\boldf \sim p(\boldf | \boldg, \lambda) = \mathcal{N}(\bmu_f,\bSigma_f)$
            \State \quad where $\bSigma_f = \left(K^{-1} + \lambda\diag(\sigma(\boldg))\right)^{-1}$ and $\bmu_f = \bSigma_f\left(K^{-1}\bmu_f^0 + \lambda\diag(\sigma(\boldg))\frac{\boldy}{2}\right)$
        \EndFor
    \end{algorithmic}
    \label{alg:gibbs_hetero}
\end{algorithm}

\begin{algorithm}[H]
    \caption{\ac{CAVI} Updates for the Heteroscedastic Gaussian likelihood}
    \begin{algorithmic}
        \State \textbf{input:} $q(\boldf,\boldg) = \mathcal{N}(\boldf|\bmu_f,\bSigma_f)\mathcal{N}(\boldg|\bmu_g,\bSigma_g)$, $p(\boldf,\boldg) = \mathcal{N}(\boldf|\bmu^0_f, K)\mathcal{N}(\boldg|\bmu^0_g, K)$, $\boldy$ and $\lambda$.
        \While{convergence criteria is not met}
            \State $\psi^i = \widetilde{\sigma}(q(g^i))$
            \State $\lambda = \frac{N}{\sum_{i=1}^N (1 - \psi^i)\sqrt{(y^i - \mu_f^i)^2 + \Sigma_f^{ii}}}$
            \State $\gamma^i = \frac{\lambda}{2} \psi^i  \sqrt{(y^i - \mu_f^i)^2 + \Sigma_f^{ii}}$
            \State $c^i = \sqrt{(\mu_g^i)^2 + \Sigma^{ii}_g}$
            \State $\theta^i = \expec{q(\omega^i|n^i)q(n^i)}{\omega^i} = \frac{0.5 + \gamma^i}{2c^i}\tanh\left(\frac{c^i}{2}\right)$
            \State $\bSigma_f = \left(K^{-1} + \lambda\diag(1-\boldsymbol{\psi})\right)^{-1}$
            \State $\bmu_f = \bSigma_f\left(K^{-1}\bmu_f^0 + \lambda\diag(1 - \boldsymbol{\psi}) \boldy\right)$
            \State $\bSigma_g = \left(K^{-1} + \diag(\btheta)\right)^{-1}$
            \State $\bmu_g = \bSigma_g\left(K^{-1}\bmu_g^0 + \frac{0.5 + \bgamma}{2}\right)$
        \EndWhile
    \end{algorithmic}
    where $q(\boldn, \bomega) = \prod_{i=1}^N \PG(\omega^i|0.5 + n, c^i)\Po(n^i|\gamma^i)$ and $\widetilde{\sigma}(q(g^i)) = \frac{e^{-\mu_g^i/2}}{\sqrt{(\mu_g^i)^2 + \Sigma^{ii}_k} / 2}$ can be seen as a close approximation to $\expec{q(g^i)}{\sigma(-g^i)}$.
    \label{alg:cavi_hetero}
\end{algorithm}

A 1-dimensional toy example is shown in Figure~\ref{fig:heteroscedastic} with the results of the inference algorithms.

\begin{figure}[H]
\includegraphics[width=\textwidth]{./chapters/8_discussions/figures/heteroscedastic.pdf}
\caption{Toy example of a heteroscedastic Gaussian regression problem and the resulting inference from Algorithms~\ref{alg:cavi_hetero} and~\ref{alg:gibbs_hetero}.
The left plots show the data in orange and the generating Gaussians (mean with one standard deviation in blue), the green bands show the predictive distributions obtained after posterior inference.
The right plots show the true latent functions $f$ and $g$ used to generate $y$ as well as the inferred posteriors: variational on top (mean with one standard deviation) and samples at the bottom.}
\label{fig:heteroscedastic}
\end{figure}

Note that an implementation as well as detailed derivations can be found in the \href{https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl}{AugmentedGPLikelihoods.jl} package \cite{theo_galy_fajou_2022_6347022}.

\subsection{Heteroscedastic Non-Gaussian Likelihood}

This method extends to non-Gaussian likelihoods as well.
We take the example of the heteroscedastic Student-t likelihood, where we have a local scale with standard deviation $\epsilon(x) = \lambda \sigma(g)$ with $\lambda \in \mathbb{R}^+$.

Similar to Equation~\eqref{eq:hetero_lik}, we get the likelihood:
\begin{align}
    p(y|f,g,\lambda,\nu) = \frac{\Gamma(\frac{\nu+1}{2})\sqrt{\lambda\sigma(g)}}{\Gamma(\frac{\nu}{2})\sqrt{\pi\nu}}\left(1 + \lambda \sigma(g)\frac{(y-f)^2}{\nu}\right)^{-\frac{\nu+1}{2}}
    \label{eq:hetero_studentt}
\end{align}

To simplify the notation, we define the scaled residuals $\Delta=\Delta_\nu(f,y,\lambda) = \frac{\lambda(y-f)^2}{\nu}$, $\alpha =\frac{\nu+1}{2}$ and the normalization constant $Z = \frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2})}$.
We can proceed with the first augmentation:
\begin{align}
    (1 + \sigma(g)\Delta)^{-\alpha} =& (1 + \Delta(1 - \sigma(-g)))^{-\alpha}\nonumber\\
    =&(1 + \Delta - \Delta\sigma(-g))^{-\alpha}\nonumber\\
    =& (\Delta\sigma(-g))^{-\alpha}\left(\frac{\sigma(-g)\Delta}{1 + \Delta -\sigma(-g)\Delta}\right)\nonumber\\
    =&\sum_{k=0}^{\infty} \Delta^k \mathrm{NB}(k|\sigma(-g),\alpha),\label{eq:negativebinomial}
\end{align}
where we used the \ac{MGF} of the \textbf{Negative Binomial} distribution.

We obtain the same result by performing first the augmentation of the Student-t with a Gamma variable:
\begin{align}
    p(y|f,g,\lambda) =& \int_0^\infty \mathcal{N}(y| f, (\lambda\sigma(g)\gamma)^{-1})\mathrm{IG}(\gamma|\frac{\nu}{2},\frac{\nu}{2})d\gamma\nonumber\\
    p(y,\gamma|f,g,\lambda)= \mathcal{N}(y| f, (\lambda\sigma(g)\gamma)^{-1})\mathrm{IG}(\gamma|\frac{\nu}{2},\frac{\nu}{2}).\label{eq:aug_gamma_studentt}
\end{align}
$\mathcal{N}(y| f, (\lambda\sigma(g)\gamma)^{-1})$ is the same starting point as Equation~\eqref{eq:hetero_lik} with an additional scaling $\gamma$.
The next augmentation steps are the same as in Equation~\eqref{eq:aug_poisson} with an augmentation with a Poisson variable.
Marginalizing out the Gamma variable $\gamma$ results in a Negative Binomial distribution.

Back to Equation~\eqref{eq:negativebinomial}, we rework the likelihood by reorganizing the terms in the augmented likelihood.
\begin{align*}
    p(y,k|f,g,\lambda,\nu) =& Z\sqrt{\lambda}\sigma^{\frac{1}{2}}\sigma(-g)^{-\alpha}\Delta^{-\alpha}\Delta^k\underbrace{C(k,\alpha)\sigma^{\alpha}(g)\sigma(-g)^k}_{\mathrm{NB}(k|\sigma(-g),\alpha)}\\
    =& ZC(k,\alpha)\sqrt{\lambda}(\sigma(g))^{\frac{1}{2}+\alpha}(\sigma(-g))^{k-\alpha}\Delta^{k-\alpha}
\end{align*}
where $C(k, \alpha) = \frac{\Gamma(r + k)}{k!\Gamma(r)}$ is the normalization constant of the negative binomial.
We set $Z'=ZC(\alpha,k)\sqrt{\lambda}$ as a constant independent of $f$ or $g$.
The final step is the P\'olya-Gamma augmentation:
\begin{align}
    p(y,k,\omega|f,g,\lambda,\nu) = Z'\Delta^{k-\alpha}2^{-(\half+k)}\exp\left(\half\left(\half + 2\alpha - k\right)g + g^2\omega\right)\PG(\omega|\half+k,0)\label{eq:augmented_studentt}.
\end{align}

Like for the heteroscedastic Gaussian likelihood, the augmented likelihood~\eqref{eq:augmented_studentt} is conjugate in $g$ but not in $f$.
We can find the collapsed conditional for $f$ in closed-form.

The key to perform inference on this augmented model, is to use the right augmented likelihood for each variable.
For example, for $f$ and $\lambda$, we only want to use the Inverse Gamma augmentation described in Equation~\eqref{eq:aug_gamma_studentt}.
For $\omega$, $g$, and $k$ (used as a mixture of inverse Gamma and Poisson) we will use the fully augmented likelihood~\eqref{eq:augmented_studentt}.
This will give a combination of collapsed conditionals and full-conditionals directly usable in a Gibbs sampling scheme.
For the \ac{CAVI} updates, we reuse the double bound idea of Section~\ref{sec:hetero_gaussian}.

The full derivations, resulting algorithms and implementation can be found in the  \href{https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl}{AugmentedGPLikelihoods.jl} package \cite{theo_galy_fajou_2022_6347022}.

\section{Using Hamilton Monte Carlo on the augmented model}
The Gibbs sampler in the experiments of Chapters~\ref{ch:classification} and \ref{ch:general} outperforms the state-of-the-art \ac{HMC} algorithm introduced in Section~\ref{sec:hmc}.
A recurrent question I got is:
Is the performance gain due only to the augmentation or the Gibbs sampling scheme?
To answer this question, we try using the \ac{HMC} algorithm on augmented models.

Before doing any experiments, let us consider the consequences that the augmented model has on the \ac{HMC} sampler.
First, the augmentation increases the dimensionality of the model.
For $N$ observations, we need $KN$ more dimensions (where $K$ depends on the model); therefore, gradient computations and algorithm tuning should be more expensive.
On the other hand, since the likelihood is simplified to a quadratic problem, the computational complexity of each step can decrease!
The second issue with using \ac{HMC} on the augmented model is that the \ac{pdf} of the prior distribution on augmented variables is not always available in closed-form or not usable at all.
For example, one approximates the probability of a P\'olya-Gamma variable with a truncated alternating series,
Truncated series are computationally expensive and can also be biased and unstable!
My experience with the P\'olya-Gamma variables is that even when using tricks to improve numerical stability like the logsumexp function, the \ac{pdf} approximation can be negative, breaking the computations.
Finally, the critical problem with \ac{HMC} is that it only works with continuous variables.
Some augmentations directly involve discrete variables like the Poisson in the multi-class setting, making it incompatible with a scheme involving only \ac{HMC}.

We try running \ac{HMC} and \ac{NUTS} with a compatible augmentation (augmented variable \ac{pdf} known in closed-form, no discrete variables).
Figure~\ref{fig:hmc_vs_gibbs} shows the auto-correlation plots on \ac{GP} regression problem with a Student-t likelihood with $\nu=3$ degrees of freedom applied on the Boston housing dataset (506 data points, 13 dimensions) \cite{harrison1978hedonic}.
We draw one chain of 2000 samples (plus 500 adaptation samples for \ac{HMC} and \ac{NUTS}) for both the original and augmented model.

From the first look, \ac{HMC} applied on the augmented model has a lower auto-correlation.
When using \ac{NUTS}, the gain becomes less clear.
Moreover, the algorithm produces antithetic chains, making it harder to have a proper comparison.
The Gibbs sampler has the smallest intra-chain correlation, but one could argue that negative correlations are desirable to compute expectations.
However, \ac{HMC} and \ac{NUTS} turned out to be much slower than the Gibbs sampler:
the Gibbs sampler took around 20 sec to run against an average of 12 minutes for \ac{HMC} and \ac{NUTS}.
Perhaps surprisingly, there was no significant time difference between the augmented and original models for \ac{HMC} and \ac{NUTS}.

We should only consider these results preliminary since we used a simple likelihood, and the dataset is relatively small and easy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{./chapters/8_discussions/figures/autocorrelation.pdf}
    \caption{Auto-correlation function of the Gibbs sampler, \ac{HMC} and \ac{NUTS} on the augmented model, and \ac{HMC} and \ac{NUTS} on the original model.
    The mean is shown with one standard-deviation over all dimensions.}
    \label{fig:hmc_vs_gibbs}
\end{figure}

\section{Improvements on the Multi-Class Classification}
\label{sec:improvemulticlass}
We recently figured out additional ways to improve the multi-class classification model and the associated inference.
We present them here in 3 different sections.

\subsection{Marginalizing out variables}
\label{sec:marg_multiclass}
In the augmentation derived in Chapter~\ref{ch:multi-class}, we add $2K + 1$ new variables per observation: $\lambda$, $\{n_i\}_{i=1}^K$ and $\{\omega_i\}_{i=1}^K$.
However, we can reduce this number to $2K$ and avoid unnecessary inner loops by marginalizing out $\lambda$.
When deriving the augmentations, one ends up with the following augmented likelihood:
\begin{align}
    p(y=k, \{n_j\}_{j=1}^K, \lambda|\{f_j\}_{j=1}^K) = \sigma(f_k) \prod_{j=1}^K \sigma(-f_j)^{n_j} \Po (n_j|\lambda),
\end{align}
where we omitted the improper prior $1_{[0,\infty)}$ on $\lambda$.
We can marginalize out $\lambda$:
\begin{align}
    \int_{0}^\infty \prod_{j=1}^K \sigma(-f_j)^{n_j} \Po (n_j|\lambda)d\lambda =& \frac{1}{\prod_{j=1}^K n_j!}\int_{0}^\infty \lambda^{\sum_{j=1}^K n_j}e^{-K\lambda}d\lambda \nonumber\\
    =& \frac{K^{-\sum_{j=1}^K n_j}}{\prod_{j=1}^K n_j!}\prod_{j=1}^K \sigma(-f_j)^{n_j} \int_{0}^\infty (K\lambda)^{\sum_{j=1}^K n_j}e^{-K\lambda}d\lambda \nonumber\\
    =& \prod_{j=1}^K\sigma(-f_j)^{n_j}\Gamma(1 + \sum_{j=1}^K n_j) \prod_{j=1}^K \left(\frac{1}{K}\right)^{n_j}\frac{1}{n_j!}. \label{eq:NM}
\end{align}

Which is proportional to a negative multinomial $\operatorname{NM}(x_0, \boldsymbol{p})$ defined by:
\begin{align*}
    \operatorname{NM}(\bx|x_0,\boldsymbol{p}) = \Gamma\left(\sum_{j=0}^K x_j\right) \frac{p_0^{x_0}}{\Gamma(x_0)}\prod_{j=1}^K \frac{p_j^{x_j}}{x_j!}   
\end{align*}
with parameters $x_0=1$, $\boldsymbol{p}=\left\{\frac{\sigma(-f_j)}{K}\right\}_{j=1}^K$, and where $p_0 = 1 - \sum_{j=1}^K p_j$.
Note that the normalization term $p_0$ is missing in Equation~\eqref{eq:NM}.
However, we do not add it, as it would render the likelihood unusable.
We keep the prior unnormalized, but this does not influence the inference, as in Chapter~\ref{ch:multi-class}, since all full-conditionals are available in closed-form and normalized.

These derivations could have been avoided by noticing that the \ac{MGF} of a negative binomial distribution is given by:
\begin{align*}
    \operatorname{MGF}_{\operatorname{NM}(x_0,\boldsymbol{p})}(\boldsymbol{t}) = \left(\frac{p_0}{1-\sum_{j=1}^K p_j e^{t_j}}\right)^{x_0}.
\end{align*}


Both the Gibbs sampling and \ac{CAVI} updates based on this marginalization are described in Algorithms~\ref{alg:gibbs_multiclass} and~\ref{alg:cavi_multiclass}.


\subsection{A new model for the multi-class classification}
\label{sec:simplex}
In Chapter \ref{ch:multi-class}, two concerns can be raised.
First, the parametrization of a categorical distribution with $K$ categories requires only $K-1$ independent parameters $\boldsymbol{p}$ due to the constraint $\sum_{j=1}^K p_j = 1$.
However, in the original model, which we will call \textbf{over-parametrized}, we consider $K$ independent parameters.
Second, the augmented variable $\lambda$ has the improper prior $p(\lambda) = 1_{[0,\infty)}$, which is a proper measure but is not normalizable.
It is not an important concern since \textbf{the posterior is normalizable} despite the improper prior.
Nevertheless, one might argue that improper priors should be avoided, as it does not allow model comparison.

On a side note, the fact that augmentations with improper priors still lead to valid inference is a good indication that scale mixtures for augmentation can be extended to non-normalizable measures.

These two issues seem connected, but we do not have any proof for it.

We propose an alternative parametrization with $K-1$ latent \ac{GPs}.
The likelihood stays the same but with one latent being fixed:
\begin{align}
    p(y=k|\{f_j\}_{j=1}^{K-1}) = \left\{
        \begin{array}{cc}
            \frac{\sigma(f_k)}{D + \sum_{j=1}^{K-1}\sigma(f_j)}, & \mathrm{if}\; 1 \leq k < K - 1\\
            \frac{D}{D + \sum_{j=1}^{K-1}\sigma(f_j)}, & \mathrm{if}\; k = K - 1 \\
    \end{array}
    \right.,\label{eq:simplex_multiclass}
\end{align}
where $D = \sigma(f_K) \in [0, 1]$.
We call this version of the likelihood \textbf{bijective} since the dimensionality of the simplex output is the same as the inputs.

This likelihood comes with different properties.
Unlike the softmax link, the logistic-softmax link is not translation invariant\footnote{There is no function $f(\Delta)$ such that $\sigma(x + \Delta) = f(\Delta)\sigma(x)$ for all $x$.}.
We can not freely exchange classes, and the "fixed" class has a different behavior than the rest.
For example, since we fix $D$, the probability for classes other than $K$ will be upper bounded by $\frac{1}{D + 1}$.
For example, taking $D=0.5$ leads to a maximum probability of $1$ for the class $K$ and $2/3$ for all other classes.
On the other hand, if $D=0$, the probability of the class $K$ will always be $0$.
The bijective likelihood can still be practical if we do not care about one of the classes.
Additionally, the scaled model presented in the next Section~\ref{sec:scale_multiclass} can also help with the imbalance between classes.

Starting from the likelihood in Equation~\ref{eq:simplex_multiclass} the first augmentation that led to an improper prior in the over-parametrized model of Chapter~\ref{ch:multi-class}:
\begin{align*}
    \frac{1}{\sum_{j=1}^{K} \sigma(f_j)} = \int_0^\infty e^{-\lambda  \sum_{j=1}^{K} \sigma(f_j)}d\lambda
\end{align*}
is replaced by the known \ac{MGF} of a Gamma distribution with the following mixture:
\begin{align*}
    \frac{1}{D + \sum_{j=1}^{K-1} \sigma(f_j)} =& \frac{1}{D + \sum_{j=1}^{K-1} \sigma(f_j)} = \frac{1}{D}\frac{1}{1+\frac{1}{D}\sum_{j=1}^{K=1}\sigma(f_j)}\\
    =& \frac{1}{D}\int_0^\infty e^{-\lambda \sum_{j=1}^{K-1} \sigma(f_j)}\Ga \left(\lambda|1, \frac{1}{D}\right)d\lambda,
\end{align*}
which is true for $D > 0$.

The next augmentations steps are the same for the bijective and over-parametrized models:
We use the \ac{MGF} of the Poisson distribution and finally the P\'olya-Gamma augmentation.
We show the whole derivations on Algorithms~\ref{alg:gibbs_multiclass} and~\ref{alg:cavi_multiclass} and show an example on Figure~\ref{fig:bijective_multiclass}.
\begin{algorithm}[H]
    \caption{Gibbs sampling updates: $\textcolor{sb1}{K}/\textcolor{sb2}{K - 1}$ latent \ac{GPs} for $K$ classes}
    \begin{algorithmic}
    \State \textbf{input:} $\boldsymbol{F} = \{\boldf_k\}_{k=1}^K$, $p(\boldsymbol{F}) = \prod_{k=1}^{\textcolor{sb1}{K}/\textcolor{sb2}{K - 1}} p(\boldf_k|\bmu_0, K_{XX})$, $Y=\{\boldy^i\}_{i=1}^N$ (one-hot encoded)
    \For{$t$ in 1: \# samples}
        \State Draw $\boldsymbol{n}^i \sim p(\boldsymbol{n}^i|\boldsymbol{F}) = \mathrm{NM}(1, \boldsymbol{p}^i)$ where 
        $p_k^i = \textcolor{sb1}{\frac{\sigma(-f_k^i)}{K}}/\textcolor{sb2}{\frac{\sigma(-f_k^i)}{D + K - 1}}$
        \State Draw $\omega_k^i \sim p(\omega_k^i|f_k^i, n_k^i, y_k^i) = \PG(y_k^i + n^i_k, |f_k^i|)$
        \State Draw $\boldf_k \sim p(\boldf_k|\bomega_k,\boldsymbol{n}_k, \boldsymbol{Y}) = \mathcal{N}(\boldm_k, \boldsymbol{S}_k)$
        \State where $\boldsymbol{S}_k = \left(K^{-1} + \diag(\bomega_k)\right)^{-1}$ and $\boldm_k = \boldsymbol{S}_k\left(K^{-1}\bmu_0 + \frac{\boldy_k - \boldsymbol{n}_k}{2}\right)$
    \EndFor
    \end{algorithmic}
    \label{alg:gibbs_multiclass}
\end{algorithm}

\begin{algorithm}[H]
    \caption{\ac{CAVI} updates: $\textcolor{sb1}{K}/\textcolor{sb2}{K - 1}$ latent \ac{GPs} for $K$ classes}
    \begin{algorithmic}
    \State \textbf{input:} $q(\boldsymbol{F}) = \prod_{k=1}^{\textcolor{sb1}{K}/\textcolor{sb2}{K - 1}} q(\boldf_k|\bmu_k, \bSigma_k)$, $p(\boldsymbol{F} = \prod_{k=1}^{\textcolor{sb1}{K}/\textcolor{sb2}{K - 1}} p(\boldf_k|\bmu_0, K)$, $Y=\{\boldy^i\}_{i=1}^N$ (one-hot encoded)
    \While{convergence criteria is not met}
        \State $c^i_k = \sqrt{(\mu^i_k)^2 + \Sigma_k^{ii}}$
        \State $p^i_k = \textcolor{sb1}{\frac{\widetilde{\sigma}(q(f_k^i))}{K}}/\textcolor{sb2}{\frac{\widetilde{\sigma}(q(f_k^i))}{D + K - 1}}$
        \State $\bgamma^i = \expec{q(\boldsymbol{n}^i)}{\boldsymbol{n}^i} = \frac{\boldsymbol{p}^i}{1 - \sum_{i=1}^K p^i_k}$
        \State $\theta_k^i = \expec{q(\omega_k^i)}{\omega_k^i} = \frac{y_k^i + \gamma_k^i}{2c_k^i}\tanh\left(\frac{c_k^i}{2}\right)$
        \State $\bSigma_k = \left(K^{-1} + \diag(\boldsymbol{\theta}_k)\right)^{-1}$
        \State $\bmu_k = \bSigma_k\left(K^{-1}\bmu_0 + \frac{\boldy_k - \bgamma_k}{2}\right)$
    \EndWhile
    \end{algorithmic}
    where $q(\boldsymbol{N}, \bOmega) = \prod_{i=1}^N \PG(\bomega^i|\boldy^i + \boldsymbol{n}^i, \boldsymbol{c}^i)\mathrm{NM}(\boldsymbol{n}^i|1, \boldsymbol{p}^i)$ and $\widetilde{\sigma}(q(f_k^i)) = \frac{e^{-\mu_k^i/2}}{\sqrt{(\mu_k^i)^2 + \Sigma^{ii}_k} / 2}$ is an approximation to the $\sigma(-f^i_k)$.
    \label{alg:cavi_multiclass}
\end{algorithm}

We show 1-dimensional examples with 3 classes with and without the bijection on Figure~\ref{fig:bijective_multiclass} and~\ref{fig:multiclass}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{./chapters/8_discussions/figures/categorical_bijective.pdf}
    \caption{Illustration of Algorithms~\ref{alg:gibbs_multiclass} and~\ref{alg:cavi_multiclass} with the bijective link introduced in Section~\ref{sec:simplex} and the marginalization of Section~\ref{sec:marg_multiclass}.
    Each color represents a class, and we compare the true process to the inferred one for both Gibbs sampling and variational inference.
    The solid lines represent the true probabilities and latent \ac{GPs}.
    The plots on top show the variational inference results, with the expected predictive probability on the left and the variational posterior on the right.
    The plots at the bottom show the probabilities and latent GPs obtained via Gibbs sampling.}
    \label{fig:bijective_multiclass}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{./chapters/8_discussions/figures/categorical_nonbijective.pdf}
    \caption{Illustration of Algorithms~\ref{alg:gibbs_multiclass} and~\ref{alg:cavi_multiclass} with the non-bijective link with the marginalization of Section~\ref{sec:marg_multiclass}.
    Each color represents a class, and we compare the true process to the inferred one for both Gibbs sampling and variational inference.
    The solid lines represent the true probabilities and latent \ac{GPs}.
    The plots on top show the variational inference results, with the expected predictive probability on the left and the variational posterior on the right.
    The plots at the bottom show the probabilities and latent GPs obtained via Gibbs sampling.}
    \label{fig:multiclass}
\end{figure}

\subsection{Scaling the logistic-softmax link}
\label{sec:scale_multiclass}
The logistic-softmax link has issues with the predictive probabilities, in particular with many classes.
Because of the boundedness of the logistic function, the logistic-softmax link needs large values of $f_i$ to reach prediction probabilities close to $1$.
Even when the model should be very confident about a prediction and the latent \ac{GPs} are correctly inferred, the predictive probability for the correct class will be around $(1-\epsilon) / ((K - 1) \epsilon +  1 - \epsilon)$ where $\epsilon$ is the minimum value taken by $\sigma(f)$.
With a \ac{GP} prior centered at 0 and a reasonable kernel variance, $f$ can not take large values.
For example, taking 10 classes, if we assume $f_y=4$ for the correct class and $-4$ for the others, $\epsilon \approx 1.8e-2$, which gives a probability of $0.858$ with the logistic-softmax link against $0.996$ for the softmax link.

This can be solved by using a scaled logistic function.
We add $K$ hyperparameters $\btheta=\{\theta_{i}\}_{i=1}^K$ such that the likelihood becomes
\begin{align*}
    p(y=k|\{f_j\}_{j=1}^K,\btheta) = \frac{\theta_k \sigma(f_k)}{\sum_{j=1}^K \theta_j \sigma(f_j)}.
\end{align*}
The $\btheta$ parameters can be optimized using the \ac{ELBO} with the other hyperparameters.
These can also provide information about each class, a high $\theta_j$ meaning that the $j$-th class has zones of very high confidence.
With the likelihood augmented with the variable $\lambda$, the collapsed-conditional and the maximum-likelihood optimum of $\btheta$ is available in closed-form.
The maximum-likelihood optimizer is given by:
\begin{align*}
    \theta^*_k = \frac{\sum_{i=1}^N \delta(y^i, k)}{\sum_{i=1}^N \expec{q(\lambda^n)}{\lambda^n}(1 - \widetilde{\sigma}(q(f_k^i)))},
\end{align*}
where $\delta(x,y)$ is the delta function, equal to 1 if $x==y$ and 0 otherwise and where $\widetilde{\sigma}(q(f_k^i))$ is defined as in Algorithm~\ref{alg:cavi_multiclass}.
We used the model definition where $\lambda$ is not marginalized out.

By putting a prior $\mathrm{Ga}(\theta_k|\alpha,\beta)$, the collapsed conditional of each $\theta_k$ is given by:
\begin{align*}
    p(\theta_k|\boldf_k, \boldsymbol{\lambda}) = \mathrm{Ga}(\theta_k|\alpha + \sum_{i=1}^N\delta(y^i, k), \beta + \sum_{i=1}^N\lambda^i\sigma(f_k^i))
\end{align*}

A Julia implementation as well as detailed derivations can be found in the \href{https://github.com/JuliaGaussianProcesses/AugmentedGPLikelihoods.jl}{AugmentedGPLikelihoods.jl} package \cite{theo_galy_fajou_2022_6347022}.

\section{Sampling from a sparse augmented model}

Another work in progress regards the sampling of sparse \ac{GPs} models.
Sampling from the augmented model proves to be very effective (see Chapter~\ref{ch:general}) while still producing samples from the posterior $p(\boldf|\boldy)$ of the original model.
Unfortunately, this property does not transfer when using sparse \ac{GPs} (see Section~\ref{sec:sparsegps}) and the scalability is limited.
Naively using an additional set of points, without using the \citet{Titsias2009} assumption ($q(\boldu,\boldf)=q(\boldu)\prod_i p(f_i|\boldu)$), leads to an algorithm with complexity $\mathcal{O}((N+M)^3)$, defeating the purpose of using inducing points.
To solve this problem we propose to mix the Gibbs sampling approach we presented in Chapter~\ref{ch:general} with variational inference.

\citet{hensmanMCMCVariationallySparse2015} shows that the optimal variational free-form distribution of the inducing variables $\boldu$ is given by:
\begin{align}
    \log q^*(\boldu) = \sum_{i}\expec{p(f_i|\boldu)}{\log p(\boldy|f_i)} + \log p(\boldu) + C,\label{eq:obj_hensman}
\end{align}
where $C$ is an intractable constant.
We can sample from this objective and create an empirical variational distribution $q(\boldu) = \frac{1}{S}\sum_{s=1}^S \delta(\boldu,\boldu^s)$, where $\boldu^s \sim q^*(\boldu)$.
The straightforward option is to use \ac{HMC} as in \cite{hensmanMCMCVariationallySparse2015} to sample from $q^*(\boldu)$~\eqref{eq:obj_hensman}.
However, we can also derive a variational Gibbs sampling algorithm to draw samples from this objective by building on augmentations.

In our setting, the optimal distribution is defined as $q^*(\boldf,\boldu,\bOmega) = p(\boldf|\boldu)q^*(\boldu, \bOmega)$, where $\bOmega$ denotes the set of augmented variables.
We are assuming independence between each $f_i$ given $\boldu$ and note that we do not assume any blocked \ac{MF} assumptions, so we do not assume independence between $\boldu$ and $\bOmega$.
$\bOmega$, $\boldu$ and $\boldf$ are sampled one after another using the optimal conditional distributions $q^*(\bOmega|\boldf)$ and $q^*(\boldu,\boldf|\bOmega)$.
Note that we sample $\boldu$ and $\boldf$ together since they are highly correlated.
To identify the optimal conditional distributions, we look at the minimizer of the conditional \ac{KL} divergence, e.g. $q^*(\bOmega|\boldf) = \arg_{q} \min \KL{q(\bOmega|\boldf)}{p(\bOmega|\boldf,\boldy)}$.
The obvious minimizer here is to set $q^*(\bOmega|\boldf)=p(\bOmega|\boldf,\boldy)$, and thanks to the augmentations, we know the full-conditional $p(\bOmega|\ldots)$ in closed-form.
We perform the same approach for $q^*(\boldu,\boldf|\bOmega)$.

The only parameters left are the hyperparameters (omitted in the previous equations), which we sample with the \ac{HMC} algorithm in a separate Gibbs sampling step.
The advantage of this approach is that no gradients are necessary (except for the hyper-parameters) as we know all the full-conditionals in closed-form, and we do not need any expectation over the log-likelihood.

Our approach opens up more possibilities over more complex likelihoods like the multi-class or heteroscedastic ones where computing expectations is a limitation.
For medium-sized datasets, this outperforms the \ac{CAVI} algorithm as it has the same convergence speed but does not suffer from the mean-field assumption on the variational parameters.
We show preliminary results on Figures~\ref{fig:magictelescope} and~\ref{fig:magictelescope_autocor} for a binary classification problem on the Magic Telescope dataset (10 dimensions, 19020 data points) \cite{bock2004methods}.
We split the data in half into a training/test set and use $M=50$ inducing points selected via the k-means++ algorithm \cite{arthur2007k}.
We compare our approach (\textbf{VI-Gibbs}) against the sampling methods \ac{HMC} (\textbf{VI-HMC}), Elliptical Slice Sampling (\textbf{VI-ESS}), and a standard \ac{VI} method (with an L-BFGS optimizer), by showing the error and test negative log-likelihood over time on Figure~\ref{fig:magictelescope}.
Each sampling method draws 10 chains of 1000 samples.
We kept the hyper-parameters fixed in this setup.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{./chapters/8_discussions/figures/magictelescope.pdf}
    \caption{Negative test log-likelihood and classification test error over time on the Magic Telescope dataset.
    The mean with one standard deviation over 10 runs is shown for each algorithm.
    }
    \label{fig:magictelescope}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{./chapters/8_discussions/figures/magictelescope_autocor.pdf}
    \caption{Intra-chain correlation against the lag for the sampling approaches.}
    \label{fig:magictelescope_autocor}
\end{figure}

We will release this work soon.

\section{Limitations}

Unfortunately, augmentations are not a silver bullet for approximate Bayesian inference.
The largest issue is naturally the limited domain of application, only a constrained set of functions can be augmented.
The idea of generalization using \ac{MGF} as mentioned in Section~\ref{sec:further} is promising but limited nonetheless.
When they exist, the identification of augmentable functions in a given model can be tedious and may require lengthy derivations.
We often need to rearrange terms and use mathematical identities before applying procedures like the ones described in this thesis.
It is accessible to someone with expertise, but automatizing this derivation process is complicated.
Current progress in symbolic programming could eventually help in this direction.
We could automate this process by having a lookup table of augmentable functions and manipulating terms symbolically.

Another issue is the variational distribution $q(\boldf, \bOmega)$ (or $q(\boldu, \bOmega)$) optimized on the augmented model is not as accurate as the variational distribution $q(\boldf)$ (or $q(\boldu)$) optimized on the original model.\mycom{refer to chapter}
Interestingly, the bound difference comes exclusively from the mean-field assumption between $q(\boldf)$ and $q(\bOmega)$.
We can even identify these bound differences via the interpretation of \citet{jaakkolaVariationalApproachBayesian1997} as missing terms from a Taylor series.
The predictive results \mycom{what is that} prove to be almost as good, and the difference of bounds is not always significant \mycom{refer to it}.
These empirical results give us an indication that the sets of variables are naturally strongly decorrelated, which would explain why the Gibbs sampling and \ac{CAVI} updates are so efficient.
The work of \citet{gorinovaAutomaticReparameterisationProbabilistic2020} mentioned in the introduction aims at finding a perfect balance between two parametrizations by ensuring that the posterior is as close as possible to a multivariate Gaussian with independent dimensions. \mycom{unfinished}




% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
